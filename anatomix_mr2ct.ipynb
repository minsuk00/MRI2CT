{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4791ad35",
   "metadata": {},
   "source": [
    "# 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9802394e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8189ef5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "831a3ce1",
   "metadata": {},
   "source": [
    "# 1. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea48f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_gpu():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "def minmax(arr, minclip=None, maxclip=None):\n",
    "    if not (minclip is None and maxclip is None):\n",
    "        arr = np.clip(arr, minclip, maxclip)\n",
    "    arr = (arr - arr.min()) / (arr.max() - arr.min() + 1e-8)\n",
    "    return arr\n",
    "\n",
    "def find_image(root, subj_id, name):\n",
    "    base = os.path.join(root, subj_id.replace(\"_\", \"/\"))\n",
    "    matches = glob.glob(os.path.join(base, f\"{name}.mha\")) \\\n",
    "             + glob.glob(os.path.join(base, f\"{name}.nii*\"))\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(f\"{name} file not found for {subj_id}\")\n",
    "    return matches[0]\n",
    "\n",
    "def load_image_pair(root, subj_id):\n",
    "    mr_path = find_image(root, subj_id, \"mr\")\n",
    "    ct_path = find_image(root, subj_id, \"ct\")\n",
    "    mask_path = find_image(root, subj_id, \"mask\")\n",
    "\n",
    "    mr_img = tio.ScalarImage(mr_path)\n",
    "    ct_img = tio.ScalarImage(ct_path)\n",
    "\n",
    "    mri = mr_img.data[0].numpy()\n",
    "    ct  = ct_img.data[0].numpy()\n",
    "\n",
    "    mri = minmax(mri)\n",
    "    ct  = minmax(ct, minclip=-450, maxclip=450)\n",
    "    print(\"MRI shape:\", mri.shape, \"CT shape:\", ct.shape)\n",
    "    return mri, ct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b960117",
   "metadata": {},
   "source": [
    "# 2. Load dataset (one subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe277759",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/content/drive/MyDrive/Colab Notebooks/MRI2CT\"\n",
    "SUBJ_ID = \"SynthRAD2023_Task1_pelvis_1PA001\"\n",
    "\n",
    "mri, ct = load_image_pair(root, SUBJ_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12abdb3",
   "metadata": {},
   "source": [
    "# 3. Load Anatomix model and extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb6eca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet(\n",
    "    dimension=3,\n",
    "    input_nc=1,\n",
    "    output_nc=16,\n",
    "    num_downs=4,\n",
    "    ngf=16,\n",
    ").to(device)\n",
    "\n",
    "ckpt_path = \"/content/anatomix/model-weights/anatomix.pth\"\n",
    "model.load_state_dict(torch.load(ckpt_path, map_location=device), strict=True)\n",
    "print(\"âœ… Loaded anatomix pretrained model\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_feats(volume_np, model, device):\n",
    "    inp = torch.from_numpy(volume_np[None, None]).float().to(device)\n",
    "    H, W, D = inp.shape[-3:]\n",
    "    pad_H = (16 - H % 16) % 16\n",
    "    pad_W = (16 - W % 16) % 16\n",
    "    pad_D = (16 - D % 16) % 16\n",
    "    inp_padded = F.pad(inp, (0, pad_D, 0, pad_W, 0, pad_H))\n",
    "    feats = model(inp_padded)\n",
    "    feats = feats[:, :, :H, :W, :D]\n",
    "    return feats.squeeze(0).cpu().numpy()  # [C,H,W,D]\n",
    "\n",
    "feats_mri = extract_feats(mri, model, device)\n",
    "cleanup_gpu()\n",
    "feats_ct  = extract_feats(ct,  model, device)\n",
    "print(f\"âœ… MRI feats: {feats_mri.shape}, CT feats: {feats_ct.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4db152",
   "metadata": {},
   "source": [
    "# 4. Prepare voxel-wise dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfe439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(feats_mri).permute(1,2,3,0).reshape(-1, 16)\n",
    "Y = torch.from_numpy(feats_ct ).permute(1,2,3,0).reshape(-1, 16)\n",
    "print(f\"Total voxels: {len(X):,}\")\n",
    "\n",
    "max_vox = 500_000\n",
    "if len(X) > max_vox:\n",
    "    idx = torch.randperm(len(X))[:max_vox]\n",
    "    X, Y = X[idx], Y[idx]\n",
    "\n",
    "dataset = TensorDataset(X, Y)\n",
    "loader = DataLoader(dataset, batch_size=4096, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d919b73",
   "metadata": {},
   "source": [
    "# 5. Simple translator model (MLP or Conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c2f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPTranslator(nn.Module):\n",
    "    def __init__(self, in_dim=16, hidden=64, out_dim=16):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, out_dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Conv1x1Translator(nn.Module):\n",
    "    def __init__(self, in_dim=16, out_dim=16):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv3d(in_dim, out_dim, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "use_conv = False\n",
    "model_t = Conv1x1Translator(16, 16).to(device) if use_conv else MLPTranslator().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model_t.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea67a4",
   "metadata": {},
   "source": [
    "# 6. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a56b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "model_t.train()\n",
    "for epoch in range(n_epochs):\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        pred = model_t(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    print(f\"Epoch {epoch+1:02d}/{n_epochs} - Loss: {avg_loss:.6f}\")\n",
    "print(\"âœ… Training complete!\")\n",
    "\n",
    "save_path = f\"/content/drive/MyDrive/Colab Notebooks/MRI2CT/mri2ct_simple_model.pt\"\n",
    "torch.save(model_t.state_dict(), save_path)\n",
    "print(f\"ðŸ’¾ Saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efb3abc",
   "metadata": {},
   "source": [
    "# 7. Evaluate: reconstruct predicted CT features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a058f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_t.eval()\n",
    "with torch.no_grad():\n",
    "    X_full = torch.from_numpy(feats_mri).permute(1,2,3,0).reshape(-1,16).to(device)\n",
    "    pred_full = model_t(X_full).cpu().numpy()\n",
    "pred_feats = pred_full.reshape(*feats_ct.shape)  # [C,H,W,D]\n",
    "\n",
    "print(\"âœ… Predicted CT feature volume:\", pred_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c9f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c64e09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
