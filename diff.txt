diff --git a/src/preprocess/registration_batch.py b/src/preprocess/registration_batch.py
index 89f87ed..1d4aaa3 100644
--- a/src/preprocess/registration_batch.py
+++ b/src/preprocess/registration_batch.py
@@ -1,5 +1,6 @@
 import os
 import glob
+import shutil
 import time
 import numpy as np
 import pandas as pd
@@ -16,39 +17,69 @@ from anatomix.registration import convex_adam
 # ==========================================
 # 1. CONFIGURATION
 # ==========================================
-ROOT = "/home/minsukc/MRI2CT"
-DATA_DIR = os.path.join(ROOT, "data")
-CKPT_PATH = os.path.join(ROOT, "anatomix/model-weights/anatomix.pth")
+PROJECT_ROOT = "/home/minsukc/MRI2CT"
+# UPDATE: Pointing to V2 Checkpoint based on your request
+CKPT_PATH = os.path.join(PROJECT_ROOT, "anatomix/model-weights/best_val_net_G.pth")
+RES_MULT = 32 # use 16 for anatomix v1
+# CKPT_PATH = os.path.join(PROJECT_ROOT, "anatomix/model-weights/anatomix.pth")
+# RES_MULT = 16
+
+# Path to your DATA (On GPFS)
+DATA_ROOT = "/gpfs/accounts/jjparkcv_root/jjparkcv98/minsukc/MRI2CT/SynthRAD_combined/3.0x3.0x3.0mm"
 
 # Target specific subjects or set to None for ALL
 TARGET_LIST = None 
-# TARGET_LIST = ["1ABA005_3.0x3.0x3.0_resampled", "1HNA001_3.0x3.0x3.0_resampled", "1THA001_3.0x3.0x3.0_resampled"]
-# TARGET_LIST = ["1ABA005_3.0x3.0x3.0_resampled"]
+# TARGET_LIST = ["1PA021"]
+# TARGET_LIST = ["1PA112"]
 
-# *** BEST PARAMETERS FROM TUNING ***
+# *** BEST PARAMETERS ***
 BEST_PARAMS = {
     'lambda_weight': 0.5,  
-    'grid_sp': 3,           
+    'grid_sp': 2,           
     'selected_smooth': 1,   
     'selected_niter': 80,   
-    'disp_hw': 1            
+    'disp_hw': 4            
 }
 
 # ==========================================
-# 2. REGION MAPS
+# 2. REGION MAPS (Validated for TotalSegmentator V2)
 # ==========================================
 REGION_MAPS = {
     "abdomen": {
-        "Spleen": (1, 1), "Kidney_R": (2, 2), "Kidney_L": (3, 3), "Liver": (5, 5), "Stomach": (6, 6)
+        # CT (Total v2) -> MRI (Total_MR v2)
+        "Spleen": (1, 1), 
+        "Kidney_R": (2, 2), 
+        "Kidney_L": (3, 3), 
+        "Liver": (5, 5), 
+        "Stomach": (6, 6)
     },
     "thorax": {
-        "Heart": (51, 22), "Lung_L": ([10, 11], 10), "Lung_R": ([12, 13, 14], 11)
+        # Heart: CT(51) -> MR(22) [Per your provided index list]
+        "Heart": (51, 22), 
+        # Lung_L: CT(Upper 10 + Lower 11) -> MR(Left 10)
+        "Lung_L": ([10, 11], 10), 
+        # Lung_R: CT(Upper 12 + Middle 13 + Lower 14) -> MR(Right 11)
+        "Lung_R": ([12, 13, 14], 11)
     },
     "head_neck": {
+        # Brain: CT(90) -> MR(50)
+        "Brain": (90, 50)
+    },
+    "brain": {
         "Brain": (90, 50)
     },
-    "brain": {},
-    "pelvis": {}
+    "pelvis": {
+        # Soft Tissue
+        "Bladder": (21, 16),
+        "Prostate": (22, 17), # Male only (will be 0 for females)
+        
+        # Bony Structures (Critical for Pelvis Alignment)
+        "Sacrum": (25, 18),
+        "Femur_L": (75, 36),
+        "Femur_R": (76, 37),
+        "Hip_L": (77, 38),
+        "Hip_R": (78, 39)
+    }
 }
 
 
@@ -84,43 +115,58 @@ def save_nifti(arr, affine, path):
 
 def get_region_from_id(subject_id):
     if len(subject_id) < 2 or not subject_id.startswith("1"):
-        raise ValueError(
-            f"âš ï¸ Invalid subject ID '{subject_id}'. Expected format '1XX...'"
-        )
+        pass
         
     mapping = { "AB": "abdomen", "TH": "thorax", "HN": "head_neck", "B": "brain", "P": "pelvis" }
-    code_2 = subject_id[1:3].upper()
-    code_1 = subject_id[1:2].upper()
     
-    if code_2 in mapping: 
-        return mapping[code_2]
-    if code_1 in mapping: 
-        return mapping[code_1]
+    try:
+        code_2 = subject_id[1:3].upper()
+        code_1 = subject_id[1:2].upper()
+        
+        if code_2 in mapping: return mapping[code_2]
+        if code_1 in mapping: return mapping[code_1]
+    except:
+        pass
     
-    raise ValueError(
-        f"âš ï¸ Region code '{region_code}' in '{subject_id}' is not recognized..."
-    )
-
-def discover_subjects(data_dir, target_list=None):
-    if target_list:
-        candidates = target_list
-    else:
-        candidates = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])
-
-    print(f"ðŸ”Ž Scanning {len(candidates)} candidates in {data_dir}...")
-    required_files = ["mr_resampled.nii.gz", "ct_resampled.nii.gz", "mask_resampled.nii.gz", "ct_seg.nii.gz", "mr_seg.nii.gz"]
+    # Default fallback
+    print(f"âš ï¸ Warning: Could not detect region for {subject_id}, defaulting to 'abdomen' map.")
+    return "abdomen"
+
+def discover_subjects(data_root, target_list=None):
+    splits = ["train", "val", "test"]
     valid = []
+    
+    required_files = ["mr.nii.gz", "ct.nii.gz", "ct_seg.nii.gz", "mr_seg.nii.gz"]
+
+    print(f"ðŸ”Ž Scanning subjects in {data_root}...")
 
-    for subj_id in candidates:
-        subj_path = os.path.join(data_dir, subj_id)
-        missing = [f for f in required_files if not os.path.exists(os.path.join(subj_path, f))]
-        if missing:
-            if target_list: print(f"   âŒ Skipping {subj_id}: Missing {missing}")
+    for split in splits:
+        split_path = os.path.join(data_root, split)
+        if not os.path.exists(split_path):
             continue
-        region = get_region_from_id(subj_id)
-        valid.append({ "id": subj_id, "path": subj_path, "region": region })
 
-    print(f"âœ… Found {len(valid)} valid subjects.")
+        candidates = sorted([d for d in os.listdir(split_path) if os.path.isdir(os.path.join(split_path, d))])
+
+        for subj_id in candidates:
+            if target_list and subj_id not in target_list:
+                continue
+
+            subj_path = os.path.join(split_path, subj_id)
+            
+            missing = [f for f in required_files if not os.path.exists(os.path.join(subj_path, f))]
+            if missing:
+                if target_list: print(f"   âŒ Skipping {subj_id} ({split}): Missing {missing}")
+                continue
+            
+            region = get_region_from_id(subj_id)
+            valid.append({ 
+                "id": subj_id, 
+                "path": subj_path, 
+                "region": region, 
+                "split": split 
+            })
+
+    print(f"âœ… Found {len(valid)} valid subjects across all splits.")
     return valid
 
 # ==========================================
@@ -160,7 +206,6 @@ def get_target_label_mask(seg_volume, region_name):
 
 def save_registration_vis(fixed_path, moving_path, warped_path, fseg_path, mseg_path, wseg_path, disp_path, region_name, save_path):
     try:
-        # Load Data
         fix = nib.load(fixed_path).get_fdata()
         mov = nib.load(moving_path).get_fdata()
         wrp = nib.load(warped_path).get_fdata()
@@ -168,13 +213,10 @@ def save_registration_vis(fixed_path, moving_path, warped_path, fseg_path, mseg_
         fix_seg = nib.load(fseg_path).get_fdata().astype(np.int32)
         mov_seg = nib.load(mseg_path).get_fdata().astype(np.int32)
         wrp_seg = nib.load(wseg_path).get_fdata().astype(np.int32)
-        
         disp = nib.load(disp_path).get_fdata()
 
-        # Select Middle Slice
         z = int(fix.shape[2] // 2)
 
-        # Helper: Normalize and rotate
         def get_sl(vol, normalize=False):
             s = np.rot90(vol[..., z])
             if normalize:
@@ -182,17 +224,13 @@ def save_registration_vis(fixed_path, moving_path, warped_path, fseg_path, mseg_
                 if denom > 0: s = (s - s.min()) / denom
             return s
 
-        # Prepare Slices (Full Volume)
         f_sl = get_sl(fix, True)
         m_sl = get_sl(mov, True)
         w_sl = get_sl(wrp, True)
-
         fs_sl = get_sl(fix_seg)
         ms_sl = get_sl(mov_seg)
         ws_sl = get_sl(wrp_seg)
 
-        # Prepare Slices (Target Regions Only)
-        # Note: Relies on get_target_label_mask being available in global scope
         fix_filt = get_target_label_mask(fix_seg, region_name)
         mov_filt = get_target_label_mask(mov_seg, region_name)
         wrp_filt = get_target_label_mask(wrp_seg, region_name)
@@ -201,109 +239,64 @@ def save_registration_vis(fixed_path, moving_path, warped_path, fseg_path, mseg_
         ms_filt_sl = get_sl(mov_filt)
         ws_filt_sl = get_sl(wrp_filt)
 
-        # Calculate Displacement Magnitude
-        # Check dims: if 5D (x,y,z,1,3) squeeze, if 4D (x,y,z,3) use as is
         if disp.ndim == 5: disp = disp.squeeze() 
-        # Assuming channels are last (x, y, z, 3). If channels are first, use axis=0
         axis_ch = -1 if disp.shape[-1] == 3 else 0
         disp_mag_vol = np.linalg.norm(disp, axis=axis_ch)
         d_sl = get_sl(disp_mag_vol)
 
-        # Colormaps
         max_label = max(fix_seg.max(), mov_seg.max(), wrp_seg.max())
         seg_cmap = create_random_colormap(int(max_label))
         red_cmap = create_binary_colormap('red')
         green_cmap = create_binary_colormap('green')
         mag_cmap = 'jet'
 
-        # Plotting Setup (4 Rows x 4 Cols)
         fig, axes = plt.subplots(4, 4, figsize=(20, 20))
         plt.subplots_adjust(hspace=0.3, wspace=0.1)
 
-        # --- ROW 1: Intensity Inputs & Result ---
-        axes[0,0].imshow(f_sl, cmap='gray', vmin=0, vmax=1)
-        axes[0,0].set_title("Fixed (CT)")
-        
-        axes[0,1].imshow(m_sl, cmap='gray', vmin=0, vmax=1)
-        axes[0,1].set_title("Moving (MRI)")
-        
-        axes[0,2].imshow(w_sl, cmap='gray', vmin=0, vmax=1)
-        axes[0,2].set_title("Warped (Reg. MRI)")
+        axes[0,0].imshow(f_sl, cmap='gray', vmin=0, vmax=1); axes[0,0].set_title("Fixed (CT)")
+        axes[0,1].imshow(m_sl, cmap='gray', vmin=0, vmax=1); axes[0,1].set_title("Moving (MRI)")
+        axes[0,2].imshow(w_sl, cmap='gray', vmin=0, vmax=1); axes[0,2].set_title("Warped (Reg. MRI)")
+        axes[0,3].axis('off') 
 
-        axes[0,3].axis('off') # EMPTY
-
-        # --- ROW 2: Overlays & Displacement ---
-        # Pre-Registration Overlay
         rgb_pre = np.zeros((*f_sl.shape, 3))
-        rgb_pre[..., 0] = f_sl # R = CT
-        rgb_pre[..., 1] = m_sl # G = Moving
-        axes[1,0].imshow(rgb_pre)
-        axes[1,0].set_title("Pre-Reg (R=CT, G=MR)")
+        rgb_pre[..., 0] = f_sl; rgb_pre[..., 1] = m_sl
+        axes[1,0].imshow(rgb_pre); axes[1,0].set_title("Pre-Reg (R=CT, G=MR)")
 
-        # Post-Registration Overlay
         rgb_post = np.zeros((*f_sl.shape, 3))
-        rgb_post[..., 0] = f_sl # R = CT
-        rgb_post[..., 1] = w_sl # G = Warped
-        axes[1,1].imshow(rgb_post)
-        axes[1,1].set_title("Post-Reg (R=CT, G=Warped MR)")
-
-        # Displacement with Colorbar
-        im_d = axes[1,2].imshow(d_sl, cmap=mag_cmap)
-        axes[1,2].set_title("Disp. Magnitude")
+        rgb_post[..., 0] = f_sl; rgb_post[..., 1] = w_sl
+        axes[1,1].imshow(rgb_post); axes[1,1].set_title("Post-Reg (R=CT, G=Warped MR)")
+
+        im_d = axes[1,2].imshow(d_sl, cmap=mag_cmap); axes[1,2].set_title("Disp. Magnitude")
         cbar = plt.colorbar(im_d, ax=axes[1,2], fraction=0.046, pad=0.04)
         cbar.ax.tick_params(labelsize=8)
+        axes[1,3].axis('off')
 
-        axes[1,3].axis('off') # EMPTY
+        axes[2,0].imshow(fs_sl, cmap=seg_cmap, interpolation='nearest', vmin=0, vmax=max_label); axes[2,0].set_title("Fixed Seg (All)")
+        axes[2,1].imshow(ms_sl, cmap=seg_cmap, interpolation='nearest', vmin=0, vmax=max_label); axes[2,1].set_title("Moving Seg (All)")
+        axes[2,2].imshow(ws_sl, cmap=seg_cmap, interpolation='nearest', vmin=0, vmax=max_label); axes[2,2].set_title("Warped Seg (All)")
 
-        # --- ROW 3: Segmentation (ALL) ---
-        axes[2,0].imshow(fs_sl, cmap=seg_cmap, interpolation='nearest', vmin=0, vmax=max_label)
-        axes[2,0].set_title("Fixed Seg (All)")
-
-        axes[2,1].imshow(ms_sl, cmap=seg_cmap, interpolation='nearest', vmin=0, vmax=max_label)
-        axes[2,1].set_title("Moving Seg (All)")
-
-        axes[2,2].imshow(ws_sl, cmap=seg_cmap, interpolation='nearest', vmin=0, vmax=max_label)
-        axes[2,2].set_title("Warped Seg (All)")
-
-        # Mask Overlay
-        f_bin = (fs_sl > 0).astype(int)
-        w_bin = (ws_sl > 0).astype(int)
-        
+        f_bin = (fs_sl > 0).astype(int); w_bin = (ws_sl > 0).astype(int)
         axes[2,3].imshow(f_sl, cmap='gray', alpha=0.5) 
         axes[2,3].imshow(f_bin, cmap=red_cmap, interpolation='nearest', vmin=0, vmax=1)
         axes[2,3].imshow(w_bin, cmap=green_cmap, interpolation='nearest', vmin=0, vmax=1)
-        axes[2,3].set_title("All Seg Overlap (R=Fix, G=Warped MR)")
+        axes[2,3].set_title("All Seg Overlap")
 
-        # --- ROW 4: Target Regions (FILTERED) ---
-        axes[3,0].imshow(fs_filt_sl, cmap=seg_cmap, interpolation='nearest', vmin=0, vmax=max_label)
-        axes[3,0].set_title("Fixed Seg (Targets)")
-
-        axes[3,1].imshow(ms_filt_sl, cmap=seg_cmap, interpolation='nearest', vmin=0, vmax=max_label)
-        axes[3,1].set_title("Moving Seg (Targets)")
-
-        axes[3,2].imshow(ws_filt_sl, cmap=seg_cmap, interpolation='nearest', vmin=0, vmax=max_label)
-        axes[3,2].set_title("Warped Seg (Targets)")
-
-        # Target Mask Overlay
-        f_filt_bin = (fs_filt_sl > 0).astype(int)
-        w_filt_bin = (ws_filt_sl > 0).astype(int)
+        axes[3,0].imshow(fs_filt_sl, cmap=seg_cmap, interpolation='nearest', vmin=0, vmax=max_label); axes[3,0].set_title("Fixed Seg (Targets)")
+        axes[3,1].imshow(ms_filt_sl, cmap=seg_cmap, interpolation='nearest', vmin=0, vmax=max_label); axes[3,1].set_title("Moving Seg (Targets)")
+        axes[3,2].imshow(ws_filt_sl, cmap=seg_cmap, interpolation='nearest', vmin=0, vmax=max_label); axes[3,2].set_title("Warped Seg (Targets)")
 
+        f_filt_bin = (fs_filt_sl > 0).astype(int); w_filt_bin = (ws_filt_sl > 0).astype(int)
         axes[3,3].imshow(f_sl, cmap='gray', alpha=0.5)
         axes[3,3].imshow(f_filt_bin, cmap=red_cmap, interpolation='nearest', vmin=0, vmax=1)
         axes[3,3].imshow(w_filt_bin, cmap=green_cmap, interpolation='nearest', vmin=0, vmax=1)
-        axes[3,3].set_title("Target Overlap (R=Fix, G=Warped MR)")
+        axes[3,3].set_title("Target Overlap")
 
-        # Cleanup axes
         for r in range(4):
             for c in range(4):
-                if not (r==1 and c==2): # Don't turn off axis for Disp (colorbar needs it sometimes)
-                   axes[r,c].axis('off')
-                else:
-                   axes[r,c].set_xticks([])
-                   axes[r,c].set_yticks([])
-
-        plt.savefig(save_path)
-        plt.close(fig)
+                if not (r==1 and c==2): axes[r,c].axis('off')
+                else: axes[r,c].set_xticks([]); axes[r,c].set_yticks([])
+
+        plt.savefig(save_path); plt.close(fig)
     except Exception as e:
         print(f"âš ï¸ Visualization failed: {e}")
         
@@ -339,18 +332,17 @@ def compute_dice_region(gt, pred, region_name):
         scores_list.append(dice)
 
     avg = np.mean(scores_list) if scores_list else 0.0
-    print("Avg Dice:", avg)
     return organ_scores, avg
     
 # ==========================================
 # 5. MAIN PIPELINE
 # ==========================================
 def run_batch_pipeline():
-    subjects = discover_subjects(DATA_DIR, target_list=TARGET_LIST)
+    subjects = discover_subjects(DATA_ROOT, target_list=TARGET_LIST)
     if not subjects:
         return
 
-    print(f"\nðŸš€ Starting Batch Registration with Best Params: {BEST_PARAMS}")
+    print(f"\nðŸš€ Starting Batch Registration with Params: {BEST_PARAMS}")
     summary_results = []
     
     params = BEST_PARAMS.copy()
@@ -363,23 +355,22 @@ def run_batch_pipeline():
         subj_id = subj['id']
         region = subj['region']
         subj_dir = subj['path']
+        split = subj['split']
         
-        # Local output directory per subject
         result_dir = os.path.join(subj_dir, "registration_output")
+        if os.path.exists(result_dir): # force a clean directory
+            shutil.rmtree(result_dir)
         os.makedirs(result_dir, exist_ok=True)
 
-        # # If both the moved image and segmentation exist, skip this subject
-        # final_img_path = os.path.join(result_dir, "moved_mr.nii.gz")
-        # final_seg_path = os.path.join(result_dir, "labels_moved.nii.gz")
-        # if os.path.exists(final_img_path) and os.path.exists(final_seg_path):
-        #     Optional: tqmd.write(f"Skipping {subj_id} (Already done)")
-        #     continue
+        final_img_path = os.path.join(result_dir, "moved_mr.nii.gz")
+        if os.path.exists(final_img_path):
+            tqdm.write(f"Skipping {subj_id} (Already done)")
+            continue
 
-        # --- A. Load & Preprocess ---
         raw_files = {
-            'fixed': os.path.join(subj_dir, "ct_resampled.nii.gz"),
-            'moving': os.path.join(subj_dir, "mr_resampled.nii.gz"),
-            'mask': os.path.join(subj_dir, "mask_resampled.nii.gz"),
+            'fixed': os.path.join(subj_dir, "ct.nii.gz"),
+            'moving': os.path.join(subj_dir, "mr.nii.gz"),
+            'mask': os.path.join(subj_dir, "mask.nii.gz"),
             'fixed_seg': os.path.join(subj_dir, "ct_seg.nii.gz"),
             'moving_seg': os.path.join(subj_dir, "mr_seg.nii.gz"),
         }
@@ -392,18 +383,20 @@ def run_batch_pipeline():
             
             dat_fixed_norm = minmax(dat_fixed, -450, 450)
             dat_moving_norm = minmax(dat_moving)
-            dat_mask = nib.load(raw_files['mask']).get_fdata()
             dat_fseg = nib.load(raw_files['fixed_seg']).get_fdata()
             dat_mseg = nib.load(raw_files['moving_seg']).get_fdata()
 
-            # Pad
-            pad_fixed, pad_vals = pad_to_multiple_np(dat_fixed_norm, 16)
-            pad_moving, _ = pad_to_multiple_np(dat_moving_norm, 16)
-            pad_mask, _ = pad_to_multiple_np(dat_mask, 16)
-            pad_fseg, _ = pad_to_multiple_np(dat_fseg, 16)
-            pad_mseg, _ = pad_to_multiple_np(dat_mseg, 16)
+            if os.path.exists(raw_files['mask']):
+                dat_mask = nib.load(raw_files['mask']).get_fdata()
+            else:
+                dat_mask = np.ones_like(dat_fixed)
+
+            pad_fixed, pad_vals = pad_to_multiple_np(dat_fixed_norm, RES_MULT)
+            pad_moving, _ = pad_to_multiple_np(dat_moving_norm, RES_MULT)
+            pad_mask, _ = pad_to_multiple_np(dat_mask, RES_MULT)
+            pad_fseg, _ = pad_to_multiple_np(dat_fseg, RES_MULT)
+            pad_mseg, _ = pad_to_multiple_np(dat_mseg, RES_MULT)
 
-            # Temp Files
             temp_paths = {k: os.path.join(result_dir, f"temp_padded_{k}.nii.gz") for k in ['fixed', 'moving', 'mask', 'fixed_seg', 'moving_seg']}
             save_nifti(pad_fixed, affine, temp_paths['fixed'])
             save_nifti(pad_moving, affine, temp_paths['moving'])
@@ -415,7 +408,6 @@ def run_batch_pipeline():
             tqdm.write(f"   âŒ Preprocessing failed for {subj_id}: {e}")
             continue
 
-        # --- B. Run Registration ---
         try:
             convex_adam(
                 ckpt_path=CKPT_PATH,
@@ -434,26 +426,30 @@ def run_batch_pipeline():
             )
         except Exception as e:
             tqdm.write(f"   ðŸ’¥ Registration failed: {e}")
-            for p in temp_paths.values(): os.remove(p)
+            for p in temp_paths.values(): 
+                if os.path.exists(p): os.remove(p)
             continue
 
-        # --- C. Unpad & Save Final Results ---
         try:
-            # Glob search for output
             warped_pattern = os.path.join(result_dir, "moved*.nii.gz")
             label_pattern = os.path.join(result_dir, "labels_moved_temp_padded*.nii.gz")
             disp_pattern = os.path.join(result_dir, "disp_temp_padded_moving*.nii.gz")
 
-            moved_img_path = glob.glob(warped_pattern)[0]
-            moved_seg_path = glob.glob(label_pattern)[0]
-            disp_path = glob.glob(disp_pattern)[0]
+            m_imgs = glob.glob(warped_pattern)
+            m_segs = glob.glob(label_pattern)
+            m_disps = glob.glob(disp_pattern)
+            
+            if not (m_imgs and m_segs and m_disps):
+                raise FileNotFoundError("Registration output files not found.")
+
+            moved_img_path = m_imgs[0]
+            moved_seg_path = m_segs[0]
+            disp_path = m_disps[0]
 
-            # Load and Unpad
             moved_img_unpad = unpad_np(nib.load(moved_img_path).get_fdata(), pad_vals)
             moved_seg_unpad = unpad_np(nib.load(moved_seg_path).get_fdata(), pad_vals)
             disp_unpad = unpad_np(nib.load(disp_path).get_fdata(), pad_vals)
             
-            # Save Final (using original affine)
             final_img_path = os.path.join(result_dir, "moved_mr.nii.gz")
             final_seg_path = os.path.join(result_dir, "labels_moved.nii.gz")
             final_disp_path = os.path.join(result_dir, "disp.nii.gz")
@@ -461,10 +457,8 @@ def run_batch_pipeline():
             save_nifti(moved_seg_unpad, affine, final_seg_path)
             save_nifti(disp_unpad, affine, final_disp_path)
 
-            # Evaluate
             organ_scores, avg_dice = compute_dice_region(dat_fseg, moved_seg_unpad, region)
             
-            # Vis
             vis_path = os.path.join(result_dir, "registration_qc.png")
             save_registration_vis(
                 raw_files['fixed'], raw_files['moving'], final_img_path,
@@ -472,27 +466,25 @@ def run_batch_pipeline():
                 region, vis_path
             )
 
-            # Store result
             summary_results.append({
-                'subject': subj_id, 'region': region, 'avg_dice': avg_dice, **organ_scores
+                'subject': subj_id, 'region': region, 'split': split, 'avg_dice': avg_dice, **organ_scores
             })
+            tqdm.write(f"Avg Dice score for {subj_id} ({region}): {avg_dice}")
 
-            # Cleanup
             for p in temp_paths.values():
-                os.remove(p)
-            os.remove(moved_img_path)
-            os.remove(moved_seg_path)
-            os.remove(disp_path)
+                if os.path.exists(p): os.remove(p)
+            if os.path.exists(moved_img_path): os.remove(moved_img_path)
+            if os.path.exists(moved_seg_path): os.remove(moved_seg_path)
+            if os.path.exists(disp_path): os.remove(disp_path)
 
         except Exception as e:
             tqdm.write(f"   âš ï¸ Post-processing failed: {e}")
-            for p in temp_paths.values(): os.remove(p)
+            for p in temp_paths.values(): 
+                if os.path.exists(p): os.remove(p)
             continue
 
-    # Save Summary CSV
     if summary_results:
-        # Save to root data dir or project root
-        summary_path = os.path.join(DATA_DIR, "_registration_summary.csv")
+        summary_path = os.path.join(DATA_ROOT, "_registration_summary.csv")
         pd.DataFrame(summary_results).to_csv(summary_path, index=False)
         print(f"\nðŸ Batch Processing Complete. Results saved to {summary_path}")
 
diff --git a/src/preprocess/registration_tune.py b/src/preprocess/registration_tune.py
index 1fc6534..74a3ca0 100644
--- a/src/preprocess/registration_tune.py
+++ b/src/preprocess/registration_tune.py
@@ -14,15 +14,18 @@ warnings.filterwarnings("ignore", category=UserWarning, module='monai.utils.modu
 from anatomix.registration import convex_adam
 
 ROOT = "/home/minsukc/MRI2CT"
-DATA_DIR = os.path.join(ROOT, "data")
-CKPT = os.path.join(ROOT, "anatomix/model-weights/anatomix.pth")
+DATA_DIR = "/gpfs/accounts/jjparkcv_root/jjparkcv98/minsukc/MRI2CT/SynthRAD_combined/3.0x3.0x3.0mm"
+CKPT = os.path.join(ROOT, "anatomix/model-weights/best_val_net_G.pth")
+RES_MULT = 32
+# CKPT = os.path.join(ROOT, "anatomix/model-weights/anatomix.pth")
 SCRATCH_ROOT =  "/scratch/jjparkcv_root/jjparkcv98/minsukc/MRI2CT/"
 # OUTPUT_DIR = os.path.join(ROOT, "tuning_outputs")
 OUTPUT_DIR = os.path.join(SCRATCH_ROOT, "tuning_outputs")
 
 # --- 1. Define Subjects ---
 # Leave None to scan all folders in DATA_DIR that have required files
-TARGET_SUBJECTS = None 
+# TARGET_SUBJECTS = None 
+TARGET_SUBJECTS = ["1ABA005", "1BA014","1HNA013","1PA021","1THA010"]  
 # TARGET_SUBJECTS = ["1ABA005_3.0x3.0x3.0_resampled", "1HNA001_3.0x3.0x3.0_resampled", "1THA001_3.0x3.0x3.0_resampled"]
 # TARGET_LIST = ["1ABA005_3.0x3.0x3.0_resampled","1ABA009_3.0x3.0x3.0_resampled","1ABA011_3.0x3.0x3.0_resampled","1ABA012_3.0x3.0x3.0_resampled","1ABA014_3.0x3.0x3.0_resampled","1ABA018_3.0x3.0x3.0_resampled","1ABA019_3.0x3.0x3.0_resampled","1ABA025_3.0x3.0x3.0_resampled","1ABA029_3.0x3.0x3.0_resampled","1ABA030_3.0x3.0x3.0_resampled","1HNA001_3.0x3.0x3.0_resampled","1HNA004_3.0x3.0x3.0_resampled","1HNA006_3.0x3.0x3.0_resampled","1HNA008_3.0x3.0x3.0_resampled","1HNA010_3.0x3.0x3.0_resampled","1HNA012_3.0x3.0x3.0_resampled","1HNA013_3.0x3.0x3.0_resampled","1HNA014_3.0x3.0x3.0_resampled","1HNA015_3.0x3.0x3.0_resampled","1HNA018_3.0x3.0x3.0_resampled","1THA001_3.0x3.0x3.0_resampled","1THA002_3.0x3.0x3.0_resampled","1THA003_3.0x3.0x3.0_resampled","1THA004_3.0x3.0x3.0_resampled","1THA005_3.0x3.0x3.0_resampled","1THA010_3.0x3.0x3.0_resampled","1THA011_3.0x3.0x3.0_resampled","1THA013_3.0x3.0x3.0_resampled","1THA015_3.0x3.0x3.0_resampled","1THA016_3.0x3.0x3.0_resampled"]
 
@@ -35,7 +38,6 @@ grid = {
     # 3. Grid Resolution
     'grid_sp': [2, 3],
     # 4. Iterations
-    # 'selected_niter': [40, 80, 120],
     'selected_niter': [80],
     'smooth': [0, 1],
 }
@@ -50,8 +52,8 @@ REGION_MAPS = {
     "head_neck": {
         "Brain": (90, 50)
     },
-    "brain": {},
-    "pelvis": {}
+    "brain": {"Brain": (90, 50)},
+    "pelvis": {"Bladder": (21, 16), "Sacrum": (25, 18), "Femur_L": (75, 36), "Femur_R": (76, 37)}
 }
 
 # ==========================================
@@ -110,61 +112,49 @@ def get_region_from_id(subject_id):
     )
 
 def discover_tuning_subjects(data_dir, target_list=None):
-    """
-    Scans data_dir for subjects that have ALL required files for tuning.
-    Returns a list of dicts: [{'id': '...', 'region': '...'}, ...]
-    """
     valid_configs = []
+    splits = ["train", "val", "test"]
     
-    # Candidates: Either specific targets or all subdirectories
-    if target_list:
-        candidates = target_list
-    else:
-        candidates = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])
-
-    print(f"ðŸ”Ž Scanning {len(candidates)} candidates in {data_dir}...")
+    required_files = ["mr.nii.gz", "ct.nii.gz", "ct_seg.nii.gz", "mr_seg.nii.gz"]
 
-    required_files = [
-        "mr_resampled.nii.gz", "ct_resampled.nii.gz", 
-        "mask_resampled.nii.gz", "ct_seg.nii.gz", "mr_seg.nii.gz"
-    ]
+    print(f"ðŸ”Ž Scanning subjects in {data_dir}...")
 
-    for subj_id in candidates:
-        subj_path = os.path.join(data_dir, subj_id)
+    for split in splits:
+        split_path = os.path.join(data_dir, split)
+        if not os.path.exists(split_path): continue
         
-        # 1. Check if all files exist
-        missing = [f for f in required_files if not os.path.exists(os.path.join(subj_path, f))]
+        candidates = sorted([d for d in os.listdir(split_path) if os.path.isdir(os.path.join(split_path, d))])
         
-        if missing:
-            if target_list: # Only warn if user explicitly asked for this subject
-                print(f"   âŒ Skipping {subj_id}: Missing {missing}")
-            continue
+        for subj_id in candidates:
+            # Filter targets
+            if target_list and subj_id not in target_list: continue
+            
+            subj_full_path = os.path.join(split_path, subj_id)
+            missing = [f for f in required_files if not os.path.exists(os.path.join(subj_full_path, f))]
+            
+            if missing:
+                if target_list: print(f"   âŒ Skipping {subj_id}: Missing {missing}")
+                continue
 
-        # 2. Infer Region
-        region = get_region_from_id(subj_id)
-        
-        valid_configs.append({
-            "id": subj_id,
-            "region": region
-        })
+            valid_configs.append({
+                "id": subj_id,
+                "path": subj_full_path, # Keep full path
+                "region": get_region_from_id(subj_id)
+            })
 
     print(f"âœ… Found {len(valid_configs)} valid subjects for tuning.")
     return valid_configs
 
-def prepare_subject_data(data_dir, subj_id):
-    """
-    Loads raw resampled files, applies minmax & padding, saves temporary files.
-    Returns a dictionary of paths to the temporary padded files.
-    """
-    target_dir = os.path.join(data_dir, subj_id)
+def prepare_subject_data(subj_conf):
+    target_dir = subj_conf['path']
+    subj_id = subj_conf['id']
     
-    # 1. Define Raw Paths (Input)
     raw_paths = {
-        'mr': os.path.join(target_dir, "mr_resampled.nii.gz"),
-        'ct': os.path.join(target_dir, "ct_resampled.nii.gz"),
-        'mask': os.path.join(target_dir, "mask_resampled.nii.gz"),
+        'mr': os.path.join(target_dir, "mr.nii.gz"),
+        'ct': os.path.join(target_dir, "ct.nii.gz"),
         'ct_seg': os.path.join(target_dir, "ct_seg.nii.gz"),
         'mr_seg': os.path.join(target_dir, "mr_seg.nii.gz"),
+        'mask': os.path.join(target_dir, "mask.nii.gz"), 
     }
     
     print(f"   Processing {subj_id}: Normalizing & Padding...")
@@ -187,20 +177,23 @@ def prepare_subject_data(data_dir, subj_id):
     ct_norm = minmax(ct_data, minclip=-450, maxclip=450)
     
     # 4. Pad to multiple of 16
-    mr_pad, _      = pad_to_multiple_np(mri_norm, 16)
-    ct_pad, _      = pad_to_multiple_np(ct_norm, 16)
-    mask_pad, _    = pad_to_multiple_np(mask_data, 16)
-    ct_seg_pad, _ = pad_to_multiple_np(ct_seg_data, 16)
-    mr_seg_pad, _ = pad_to_multiple_np(mr_seg_data, 16)
+    mr_pad, _      = pad_to_multiple_np(mri_norm, RES_MULT)
+    ct_pad, _      = pad_to_multiple_np(ct_norm, RES_MULT)
+    mask_pad, _    = pad_to_multiple_np(mask_data, RES_MULT)
+    ct_seg_pad, _ = pad_to_multiple_np(ct_seg_data, RES_MULT)
+    mr_seg_pad, _ = pad_to_multiple_np(mr_seg_data, RES_MULT)
 
     # 5. Define Output Paths (Temporary)
+    temp_dir = os.path.join(target_dir, "temp_tuning")
+    os.makedirs(temp_dir, exist_ok=True)
+    
     temp_paths = {
-        'fixed':       os.path.join(target_dir, "ct_padded_temp.nii.gz"),
-        'fixed_mask':  os.path.join(target_dir, "mask_padded_temp.nii.gz"), 
-        'fixed_seg':   os.path.join(target_dir, "ct_seg_padded_temp.nii.gz"),
-        'moving':      os.path.join(target_dir, "mr_padded_temp.nii.gz"),
-        'moving_mask': os.path.join(target_dir, "mask_padded_temp.nii.gz"),
-        'moving_seg':  os.path.join(target_dir, "mr_seg_padded_temp.nii.gz"),
+        'fixed':       os.path.join(temp_dir, "ct_padded_temp.nii.gz"),
+        'fixed_mask':  os.path.join(temp_dir, "mask_padded_temp.nii.gz"), 
+        'fixed_seg':   os.path.join(temp_dir, "ct_seg_padded_temp.nii.gz"),
+        'moving':      os.path.join(temp_dir, "mr_padded_temp.nii.gz"),
+        'moving_mask': os.path.join(temp_dir, "mask_padded_temp.nii.gz"),
+        'moving_seg':  os.path.join(temp_dir, "mr_seg_padded_temp.nii.gz"),
     }
 
     # 6. Save Temporary Files
@@ -487,7 +480,7 @@ def run_tuning_session(data_dir, output_root, ckpt_path, subjects_config, param_
 
         # --- A. Preprocess (Create Temp Files) ---
         try:
-            temp_paths = prepare_subject_data(data_dir, subj_id)
+            temp_paths = prepare_subject_data(subj_conf)
         except Exception as e:
             print(f"   âŒ Preprocessing failed: {e}")
             continue
diff --git a/src/preprocess/resample_batch.py b/src/preprocess/resample_batch.py
index f959d8c..1749d01 100644
--- a/src/preprocess/resample_batch.py
+++ b/src/preprocess/resample_batch.py
@@ -4,179 +4,201 @@ import numpy as np
 import matplotlib.pyplot as plt
 import SimpleITK as sitk
 from tqdm import tqdm
+import random
 
-INPUT_DIR = "/scratch/jjparkcv_root/jjparkcv98/minsukc/SynthRAD2025/Task1"
-OUTPUT_DIR = "/home/minsukc/MRI2CT/data"
-# OUTPUT_DIR = "/scratch/jjparkcv_root/jjparkcv98/minsukc/MRI2CT/data"
+# ==========================================
+# 1. CONFIGURATION
+# ==========================================
+# Input Datasets (Add/Remove as needed)
+INPUT_DIRS = [
+    "/scratch/jjparkcv_root/jjparkcv98/minsukc/SynthRAD2025/Task1",
+    "/scratch/jjparkcv_root/jjparkcv98/minsukc/SynthRAD2023/Task1" 
+]
 
-# Number of volumes to process PER REGION. Set to None to process all.
-# NUM_VOLUMES = None  # e.g., 5 or None
-# NUM_VOLUMES = 10
-NUM_VOLUMES = 35
+# Correct Output Path
+OUTPUT_ROOT = "/gpfs/accounts/jjparkcv_root/jjparkcv98/minsukc/MRI2CT/SynthRAD_combined"
 
-# Target spacing in x y z (mm)
+# Split Ratios
+SPLIT_RATIOS = {"train": 0.7, "val": 0.1, "test": 0.2}
+
+# Target spacing (mm)
 TARGET_SPACING = [3.0, 3.0, 3.0]
-# TARGET_SPACING = [1.5, 1.5, 1.5]
+
+# Set to None to process ALL volumes
+NUM_VOLUMES = None 
+# NUM_VOLUMES = 5  # Debug mode
 
 def main():
-    # Setup directories
-    os.makedirs(OUTPUT_DIR, exist_ok=True)
-    viz_dir = os.path.join(OUTPUT_DIR, "_visualizations")
+    print(f"ðŸš€ Starting Multi-Dataset Resampling & Splitting")
+    print(f"   Target Spacing: {TARGET_SPACING}")
+    print(f"   Output Root: {OUTPUT_ROOT}")
+
+    # --- Step 1: Discover All Subjects ---
+    all_subjects = []
     
-    # Get regions (e.g., AB, HN, TH)
-    try:
-        regions = sorted([d for d in os.listdir(INPUT_DIR) if os.path.isdir(os.path.join(INPUT_DIR, d))])
-    except FileNotFoundError:
-        print(f"âŒ Error: Input directory not found: {INPUT_DIR}")
-        return
+    for input_dir in INPUT_DIRS:
+        if not os.path.exists(input_dir):
+            print(f"âŒ WARNING: Directory not found: {input_dir}")
+            continue
+            
+        print(f"ðŸ”Ž Scanning: {input_dir}...")
+        try:
+            # Assumes structure: InputDir -> Region -> PatientID
+            regions = sorted([d for d in os.listdir(input_dir) if os.path.isdir(os.path.join(input_dir, d))])
+        except Exception as e:
+            print(f"âŒ Error reading {input_dir}: {e}")
+            continue
+
+        for region in regions:
+            region_path = os.path.join(input_dir, region)
+            patients = sorted(os.listdir(region_path))
+            
+            for pid in patients:
+                p_path = os.path.join(region_path, pid)
+                if os.path.isdir(p_path):
+                    all_subjects.append({
+                        "id": pid,
+                        "path": p_path,
+                        "region": region,
+                        "dataset": os.path.basename(input_dir)
+                    })
+
+    # Debug limit
+    if NUM_VOLUMES is not None:
+        print(f"âš ï¸ DEBUG MODE: Limiting to first {NUM_VOLUMES} subjects.")
+        all_subjects = all_subjects[:NUM_VOLUMES]
+
+    total_subjs = len(all_subjects)
+    print(f"âœ… Found {total_subjs} total subjects.")
+    if total_subjs == 0: return
+
+    # --- Step 2: Shuffle & Assign Splits ---
+    # Random seed ensures the same split every time you run this script on the same data
+    random.seed(42) 
+    random.shuffle(all_subjects)
+
+    n_train = int(total_subjs * SPLIT_RATIOS["train"])
+    n_val = int(total_subjs * SPLIT_RATIOS["val"])
     
-    print(f"ðŸš€ Starting Resampling Job")
-    print(f"   Input: {INPUT_DIR}")
-    print(f"   Output: {OUTPUT_DIR}")
-    print(f"   Target Spacing: {TARGET_SPACING}")
-    print(f"   Limit per region: {'ALL' if NUM_VOLUMES is None else NUM_VOLUMES}")
-    print("-" * 50)
+    splits = {
+        "train": all_subjects[:n_train],
+        "val": all_subjects[n_train:n_train+n_val],
+        "test": all_subjects[n_train+n_val:]
+    }
+
+    # --- Step 3: Resample and Save to Split Folders ---
+    # Example Output: .../SynthRAD_combined/3.0x3.0x3.0mm/train/12345/
+    spacing_str = "x".join([str(s) for s in TARGET_SPACING])
+    base_out = os.path.join(OUTPUT_ROOT, f"{spacing_str}mm") 
+
+    for split_name, subjects in splits.items():
+        print(f"\nðŸ“‚ Processing SPLIT: {split_name.upper()} ({len(subjects)} volumes)")
+        
+        # Define split directory
+        split_dir = os.path.join(base_out, split_name)
+        os.makedirs(split_dir, exist_ok=True)
+        
+        # Separate viz directory per split
+        viz_dir = os.path.join(base_out, "_visualizations", split_name) 
 
-    total_processed = 0
+        for idx, subj in enumerate(tqdm(subjects)):
+            # Save visualization for the first patient in each split to check quality
+            process_subject(subj, split_dir, viz_dir, save_viz=(idx == 0))
 
-    for region in regions:
-        region_path = os.path.join(INPUT_DIR, region)
-        patients = sorted(os.listdir(region_path))
-        
-        # Filter: Take only first N patients if argument is provided
-        if NUM_VOLUMES is not None:
-            patients_to_process = patients[:NUM_VOLUMES]
-        else:
-            patients_to_process = patients
+    print("\n" + "-" * 50)
+    print("ðŸ Batch Processing Complete.")
 
-        print(f"ðŸ“‚ Processing Region {region}: {len(patients_to_process)} volumes")
+def process_subject(subj, output_dir, viz_dir, save_viz=False):
+    """
+    Reads MR/CT, resamples them, and saves to the output_dir/PatientID/
+    """
+    pid = subj['id']
+    src_path = subj['path']
+    
+    mr_path = os.path.join(src_path, "mr.nii.gz")
+    ct_path = os.path.join(src_path, "ct.nii.gz")
+    mask_path = os.path.join(src_path, "mask.nii.gz")
 
-        for idx, patient_id in enumerate(tqdm(patients_to_process)):
-            patient_dir = os.path.join(region_path, patient_id)
-            
-            mr_path = os.path.join(patient_dir, "mr.nii.gz")
-            ct_path = os.path.join(patient_dir, "ct.nii.gz")
-            mask_path = os.path.join(patient_dir, "mask.nii.gz")
-
-            # Check validity
-            if not (os.path.exists(mr_path) and os.path.exists(ct_path)):
-                continue
-
-            # Create Output Directory
-            spacing_str = "x".join([str(s) for s in TARGET_SPACING])
-            out_patient_dir = os.path.join(OUTPUT_DIR, f"{patient_id}_{spacing_str}_resampled")
-            os.makedirs(out_patient_dir, exist_ok=True)
-
-            out_mr_path = os.path.join(out_patient_dir, "mr_resampled.nii.gz")
-            out_ct_path = os.path.join(out_patient_dir, "ct_resampled.nii.gz")
-            # Skip if both main files already exist
-            if os.path.exists(out_mr_path) and os.path.exists(out_ct_path):
-                print(f"Skipping {patient_id} (already done)") # Optional log
-                continue
-
-            # --- Resample ---
-            # Returns (original_array, resampled_array)
-            mr_orig, mr_res = resample_volume(mr_path, out_mr_path, TARGET_SPACING)
-            ct_orig, ct_res = resample_volume(ct_path, out_ct_path, TARGET_SPACING)
-            
-            mask_orig, mask_res = None, None
-            if os.path.exists(mask_path):
-                mask_orig, mask_res = resample_volume(mask_path, os.path.join(out_patient_dir, "mask_resampled.nii.gz"), TARGET_SPACING, is_mask=True)
-
-            # --- Visualize (Only first patient of the region) ---
-            if idx == 0:
-                # Prepare dictionaries for plotting function
-                # Normalize for display
-                orig_dict = {
-                    'MRI': minmax(mr_orig),
-                    'CT': minmax(ct_orig, -450, 450),
-                    'Mask': mask_orig
-                }
-                res_dict = {
-                    'MRI': minmax(mr_res),
-                    'CT': minmax(ct_res, -450, 450),
-                    'Mask': mask_res
-                }
-                save_comparison_plot(orig_dict, res_dict, patient_id, viz_dir)
-            
-            total_processed += 1
+    # Basic validation
+    if not (os.path.exists(mr_path) and os.path.exists(ct_path)):
+        return
+
+    # Create patient folder inside the split directory
+    out_patient_dir = os.path.join(output_dir, pid)
+    os.makedirs(out_patient_dir, exist_ok=True)
+
+    # Naming convention: keeping it simple or appending suffix
+    out_mr = os.path.join(out_patient_dir, "mr.nii.gz")
+    out_ct = os.path.join(out_patient_dir, "ct.nii.gz")
+    out_mask = os.path.join(out_patient_dir, "mask.nii.gz")
 
-    print("-" * 50)
-    print(f"âœ… Done! Processed {total_processed} total volumes.")
-    print(f"ðŸ“Š Visualizations saved to: {viz_dir}")
+    # Skip if done
+    if os.path.exists(out_mr) and os.path.exists(out_ct):
+        return 
 
+    try:
+        # Resample Images
+        mr_orig, mr_res = resample_volume(mr_path, out_mr, TARGET_SPACING)
+        ct_orig, ct_res = resample_volume(ct_path, out_ct, TARGET_SPACING)
+        
+        # Resample Mask (Nearest Neighbor) if exists
+        mask_orig, mask_res = None, None
+        if os.path.exists(mask_path):
+            mask_orig, mask_res = resample_volume(mask_path, out_mask, TARGET_SPACING, is_mask=True)
+
+        # QC Plot
+        if save_viz:
+            orig_dict = {'MRI': minmax(mr_orig), 'CT': minmax(ct_orig, -450, 450), 'Mask': mask_orig}
+            res_dict = {'MRI': minmax(mr_res), 'CT': minmax(ct_res, -450, 450), 'Mask': mask_res}
+            save_comparison_plot(orig_dict, res_dict, pid, viz_dir)
+
+    except Exception as e:
+        print(f"âŒ Failed on {pid}: {e}")
+
+# ==========================================
+# UTILITIES
+# ==========================================
 def minmax(arr, minclip=None, maxclip=None):
-    """Normalize array to 0-1 range with optional clipping."""
     if minclip is not None and maxclip is not None:
         arr = np.clip(arr, minclip, maxclip)
-    
-    range_val = arr.max() - arr.min()
-    if range_val == 0:
-        return np.zeros_like(arr)
-    return (arr - arr.min()) / range_val
+    denom = arr.max() - arr.min()
+    if denom == 0: return np.zeros_like(arr)
+    return (arr - arr.min()) / denom
 
 def get_middle_slice(arr):
-    """Extract the middle axial slice."""
-    # SimpleITK arrays are usually (z, y, x)
+    if arr is None: return np.zeros((100,100))
     z_mid = arr.shape[0] // 2
     return arr[z_mid, :, :]
 
 def save_comparison_plot(orig_dict, res_dict, patient_id, save_dir):
-    """
-    Saves a comparison plot of middle slices.
-    orig_dict/res_dict: {'MRI': arr, 'CT': arr, 'Mask': arr}
-    """
     os.makedirs(save_dir, exist_ok=True)
-    
     fig, axes = plt.subplots(2, 3, figsize=(15, 10))
     
-    # Row 1: Originals
-    axes[0, 0].imshow(get_middle_slice(orig_dict['MRI']), cmap='gray')
-    axes[0, 0].set_title(f"Original MRI")
-    axes[0, 1].imshow(get_middle_slice(orig_dict['CT']), cmap='gray')
-    axes[0, 1].set_title(f"Original CT")
-    if orig_dict['Mask'] is not None:
-        axes[0, 2].imshow(get_middle_slice(orig_dict['Mask']), cmap='gray')
-        axes[0, 2].set_title(f"Original Mask")
-    else:
-        axes[0, 2].axis('off')
-
-    # Row 2: Resampled
-    axes[1, 0].imshow(get_middle_slice(res_dict['MRI']), cmap='gray')
-    axes[1, 0].set_title(f"Resampled MRI")
-    axes[1, 1].imshow(get_middle_slice(res_dict['CT']), cmap='gray')
-    axes[1, 1].set_title(f"Resampled CT")
-    if res_dict['Mask'] is not None:
-        axes[1, 2].imshow(get_middle_slice(res_dict['Mask']), cmap='gray')
-        axes[1, 2].set_title(f"Resampled Mask")
-    else:
-        axes[1, 2].axis('off')
+    def plot_ax(ax, data, title):
+        ax.imshow(get_middle_slice(data), cmap='gray')
+        ax.set_title(title)
+        ax.axis('off')
 
-    for ax_row in axes:
-        for ax in ax_row:
-            ax.axis('off')
+    plot_ax(axes[0,0], orig_dict['MRI'], "Original MRI")
+    plot_ax(axes[0,1], orig_dict['CT'], "Original CT")
+    plot_ax(axes[0,2], orig_dict['Mask'], "Original Mask")
+
+    plot_ax(axes[1,0], res_dict['MRI'], "Resampled MRI")
+    plot_ax(axes[1,1], res_dict['CT'], "Resampled CT")
+    plot_ax(axes[1,2], res_dict['Mask'], "Resampled Mask")
 
     plt.suptitle(f"Patient: {patient_id}", fontsize=16)
-    save_path = os.path.join(save_dir, f"{patient_id}_comparison.png")
-    plt.savefig(save_path)
+    plt.savefig(os.path.join(save_dir, f"{patient_id}_comparison.png"))
     plt.close()
-    # print(f"ðŸ–¼ï¸ Saved visualization to {save_path}")
 
 def resample_volume(in_path, out_path, target_spacing, is_mask=False):
-    """
-    Resamples a NIfTI file to target spacing.
-    Returns: The numpy array of the resampled image.
-    """
     img = sitk.ReadImage(in_path)
-    
-    # If visualization is needed, we need the original array too
-    # SimpleITK GetArrayFromImage returns (z, y, x)
     original_array = sitk.GetArrayFromImage(img) 
 
     original_spacing = img.GetSpacing()
     original_size = img.GetSize()
 
-    # Calculate new size
+    # Compute new dimensions
     new_size = [
         int(round(osz * ospc / tspc))
         for osz, ospc, tspc in zip(original_size, original_spacing, target_spacing)
@@ -189,7 +211,7 @@ def resample_volume(in_path, out_path, target_spacing, is_mask=False):
     resample.SetOutputOrigin(img.GetOrigin())
     resample.SetOutputPixelType(img.GetPixelIDValue())
 
-    # Use Nearest Neighbor for masks to preserve integer labels, Linear for images
+    # Important: Nearest Neighbor for masks to keep integer labels
     if is_mask:
         resample.SetInterpolator(sitk.sitkNearestNeighbor)
     else:
diff --git a/src/preprocess/totalsegmentator_batch.py b/src/preprocess/totalsegmentator_batch.py
index 4f4270e..992de93 100644
--- a/src/preprocess/totalsegmentator_batch.py
+++ b/src/preprocess/totalsegmentator_batch.py
@@ -4,106 +4,67 @@ import torch
 import nibabel as nib
 import numpy as np
 import matplotlib.pyplot as plt
-from glob import glob
 from totalsegmentator.python_api import totalsegmentator
 from tqdm import tqdm
 
 # ==========================================
 # 1. CONFIGURATION
 # ==========================================
-ROOT = "/home/minsukc/MRI2CT"
-DATA_DIR = os.path.join(ROOT, "data")
+# UPDATE: Point to the output of the previous step
+ROOT = "/gpfs/accounts/jjparkcv_root/jjparkcv98/minsukc/MRI2CT/SynthRAD_combined"
+DATA_DIR = os.path.join(ROOT, "3.0x3.0x3.0mm") # The specific resolution folder
 
-# OPTION A: Define specific folders to process (leave None to process ALL)
+# Target specific subjects or None for ALL
 TARGET_LIST = None 
-# TARGET_LIST = ["1ABA005_3.0x3.0x3.0_resampled"] 
-# TARGET_LIST = ["1ABA005_3.0x3.0x3.0_resampled","1ABA009_3.0x3.0x3.0_resampled","1ABA011_3.0x3.0x3.0_resampled","1ABA012_3.0x3.0x3.0_resampled","1ABA014_3.0x3.0x3.0_resampled","1ABA018_3.0x3.0x3.0_resampled","1ABA019_3.0x3.0x3.0_resampled","1ABA025_3.0x3.0x3.0_resampled","1ABA029_3.0x3.0x3.0_resampled","1ABA030_3.0x3.0x3.0_resampled","1HNA001_3.0x3.0x3.0_resampled","1HNA004_3.0x3.0x3.0_resampled","1HNA006_3.0x3.0x3.0_resampled","1HNA008_3.0x3.0x3.0_resampled","1HNA010_3.0x3.0x3.0_resampled","1HNA012_3.0x3.0x3.0_resampled","1HNA013_3.0x3.0x3.0_resampled","1HNA014_3.0x3.0x3.0_resampled","1HNA015_3.0x3.0x3.0_resampled","1HNA018_3.0x3.0x3.0_resampled","1THA001_3.0x3.0x3.0_resampled","1THA002_3.0x3.0x3.0_resampled","1THA003_3.0x3.0x3.0_resampled","1THA004_3.0x3.0x3.0_resampled","1THA005_3.0x3.0x3.0_resampled","1THA010_3.0x3.0x3.0_resampled","1THA011_3.0x3.0x3.0_resampled","1THA013_3.0x3.0x3.0_resampled","1THA015_3.0x3.0x3.0_resampled","1THA016_3.0x3.0x3.0_resampled"]
-
 
 # --- PERFORMANCE SETTINGS ---
 DEVICE = "gpu" 
 # False = High Res (Slow, ~5 mins/scan). True = Low Res (Fast, ~30s/scan)
-FAST_MODE = False 
-# FAST_MODE = True
+# FAST_MODE = False 
+FAST_MODE = True
 
 # ==========================================
 # 2. UTILITIES
 # ==========================================
-def get_region_from_id(subject_id):
-    """
-    Parses the subject ID to determine the anatomy region.
-    Logic: Looks at the 2 characters following the leading '1'.
-    e.g., 1ABA... -> AB -> abdomen
-    """
-    # 1. Basic Format Check
-    if len(subject_id) < 2 or not subject_id.startswith("1"):
-        raise ValueError(
-            f"âš ï¸ Invalid subject ID '{subject_id}'. Expected format '1XX...'"
-        )
-
-    # 2. Extract Code
-    region_code = subject_id[1:3].upper()
-
-    # 3. Map to regions
-    mapping = {
-        # SynthRAD 2025 (2-char codes)
-        "AB": "abdomen",
-        "TH": "thorax",
-        "HN": "head_neck",
-        # SynthRAD 2023 (1-char codes)
-        "B": "brain",
-        "P": "pelvis" 
-    }
-
-    # Extract candidates
-    code_2 = subject_id[1:3].upper()
-    code_1 = subject_id[1:2].upper()
-    
-    # Match
-    if code_2 in mapping:
-        return mapping[code_2]
-    elif code_1 in mapping:
-        return mapping[code_1]
-
-    raise ValueError(
-        f"âš ï¸ Region code '{region_code}' in '{subject_id}' is not recognized..."
-    )
-
-def discover_subjects(data_dir, target_list=None):
+def discover_subjects(data_root, target_list=None):
     """
-    Scans the data directory for valid subjects containing necessary NIfTI files.
+    Scans data_root/train, data_root/val, and data_root/test for subjects.
     """
     valid_subjects = []
-    
-    # Determine candidates
-    if target_list:
-        candidates = target_list
-    else:
-        # Get all subdirectories
-        candidates = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])
+    splits = ["train", "val", "test"]
 
-    print(f"ðŸ”Ž Scanning {len(candidates)} candidates in {data_dir}...")
+    print(f"ðŸ”Ž Scanning subjects in {data_root}...")
 
-    for subj_id in candidates:
-        subj_path = os.path.join(data_dir, subj_id)
+    for split in splits:
+        split_path = os.path.join(data_root, split)
+        if not os.path.exists(split_path):
+            continue
+            
+        # Get patient IDs in this split
+        candidates = sorted([d for d in os.listdir(split_path) if os.path.isdir(os.path.join(split_path, d))])
         
-        # Define required files
-        mr_path = os.path.join(subj_path, "mr_resampled.nii.gz")
-        ct_path = os.path.join(subj_path, "ct_resampled.nii.gz")
-
-        # Check existence
-        if os.path.exists(mr_path) and os.path.exists(ct_path):
-            region = get_region_from_id(subj_id)
-            valid_subjects.append({
-                "id": subj_id,
-                "path": subj_path,
-                "region": region,
-                "mr": mr_path,
-                "ct": ct_path
-            })
-        else:
-            if target_list: # Only warn if user specifically asked for this one
-                print(f"   âŒ Missing files for {subj_id}. Skipping.")
+        for subj_id in candidates:
+            # Filter if TARGET_LIST is set
+            if target_list and subj_id not in target_list:
+                continue
+
+            subj_path = os.path.join(split_path, subj_id)
+            
+            # UPDATE: Filenames match the output of resample_and_split.py
+            mr_path = os.path.join(subj_path, "mr.nii.gz")
+            ct_path = os.path.join(subj_path, "ct.nii.gz")
+
+            if os.path.exists(mr_path) and os.path.exists(ct_path):
+                valid_subjects.append({
+                    "id": subj_id,
+                    "path": subj_path,
+                    "split": split,
+                    "mr": mr_path,
+                    "ct": ct_path
+                })
+            else:
+                if target_list: 
+                    print(f"   âŒ Missing files for {subj_id} in {split}. Skipping.")
 
     return valid_subjects
 
@@ -148,7 +109,6 @@ def save_qa_plot(ct_path, ct_seg_path, mr_path, mr_seg_path, save_path):
         plt.tight_layout()
         plt.savefig(save_path)
         plt.close(fig)
-        # print(f"   ðŸ“¸ QA Plot saved")
         
     except Exception as e:
         print(f"   âš ï¸ QA Plot Error: {e}")
@@ -161,7 +121,7 @@ def run_batch_segmentation():
     subjects = discover_subjects(DATA_DIR, target_list=TARGET_LIST)
     
     if not subjects:
-        print("âŒ No valid subjects found. Check your paths.")
+        print(f"âŒ No valid subjects found in {DATA_DIR}. Check your paths.")
         return
 
     print(f"\nðŸš€ Starting TotalSegmentator Batch for {len(subjects)} subjects")
@@ -175,43 +135,49 @@ def run_batch_segmentation():
     for subj in tqdm(subjects, desc="Processing Subjects", unit="subj"):
         start_time = time.time()
         
+        # Outputs saved in the patient's folder
         ct_out = os.path.join(subj['path'], "ct_seg.nii.gz")
         mr_out = os.path.join(subj['path'], "mr_seg.nii.gz")
         qa_path = os.path.join(subj['path'], "segmentation_qa.png")
 
         # --- CT Segmentation ---
         if os.path.exists(ct_out):
-            tqdm.write("âœ… CT Segmentation exists.")
+            # tqdm.write(f"[{subj['id']}] CT Seg exists.") 
+            pass
         else:
-            tqdm.write("ðŸ§  Running CT Seg (Task: total)...")
+            tqdm.write(f"[{subj['id']}] Running CT Seg...")
             try:
                 totalsegmentator(
                     input=subj['ct'], output=ct_out, ml=True, 
-                    task="total", fastest=FAST_MODE, device=DEVICE, quiet = True
+                    # task="total", fast=FAST_MODE, device=DEVICE, quiet=True
+                    task="total", fast=FAST_MODE, device=DEVICE
                 )
-                # quiet = True, preview = True, skip_saving = True
             except Exception as e:
-                tqdm.write(f"ðŸ’¥ CT Seg Failed: {e}")
+                tqdm.write(f"ðŸ’¥ CT Seg Failed for {subj['id']}: {e}")
 
         # --- MR Segmentation ---
         if os.path.exists(mr_out):
-            tqdm.write("âœ… MR Segmentation exists.")
+            pass
         else:
-            tqdm.write("ðŸ§  Running MR Seg (Task: total_mr)...")
+            tqdm.write(f"[{subj['id']}] Running MR Seg...")
             try:
                 totalsegmentator(
                     input=subj['mr'], output=mr_out, ml=True, 
-                    task="total_mr", fastest=FAST_MODE, device=DEVICE, quiet = True
+                    # task="total_mr", fast=FAST_MODE, device=DEVICE, quiet=True
+                    task="total_mr", fast=FAST_MODE, device=DEVICE
                 )
             except Exception as e:
-                tqdm.write(f"ðŸ’¥ MR Seg Failed: {e}")
+                tqdm.write(f"ðŸ’¥ MR Seg Failed for {subj['id']}: {e}")
 
         # --- QA Visualization ---
+        # Only creating QA plot if both segments exist and plot doesn't exist
         if os.path.exists(ct_out) and os.path.exists(mr_out):
-            if not os.path.exists(qa_path):
-                save_qa_plot(subj['ct'], ct_out, subj['mr'], mr_out, qa_path)
+            save_qa_plot(subj['ct'], ct_out, subj['mr'], mr_out, qa_path)
+            # if not os.path.exists(qa_path):
+            #     save_qa_plot(subj['ct'], ct_out, subj['mr'], mr_out, qa_path)
         
-        tqdm.write(f"âœ¨ Subject done in {time.time() - start_time:.1f}s")
+        # Optional: Print time per subject
+        tqdm.write(f"âœ¨ {subj['id']} done in {time.time() - start_time:.1f}s")
 
 if __name__ == "__main__":
     run_batch_segmentation()
\ No newline at end of file
diff --git a/src/train-sliding_window.py b/src/train-sliding_window.py
deleted file mode 100644
index 8137d82..0000000
--- a/src/train-sliding_window.py
+++ /dev/null
@@ -1,1094 +0,0 @@
-#!/usr/bin/env python3
-
-import os
-import gc
-import sys
-import time
-import random
-import warnings
-import datetime
-from types import SimpleNamespace
-from glob import glob
-import json
-from collections import defaultdict
-import copy
-
-import numpy as np
-import torch
-import torch.nn as nn
-import torch.nn.functional as F
-import matplotlib.pyplot as plt
-from sklearn.decomposition import PCA
-from tqdm import tqdm
-import nibabel as nib
-import wandb
-import torchio as tio
-from fused_ssim import fused_ssim
-from monai.inferers import sliding_window_inference
-
-from anatomix.model.network import Unet
-
-# ==========================================
-# 0. GLOBAL SETUP & UTILS
-# ==========================================
-# Enables TF32 for significantly faster training on Ampere+ GPUs
-torch.set_float32_matmul_precision('high')
-
-warnings.filterwarnings("ignore", category=UserWarning, module="monai.utils.module")
-warnings.filterwarnings("ignore", message=".*SubjectsLoader in PyTorch >= 2.3.*")
-warnings.filterwarnings("ignore", message=".*non-tuple sequence for multidimensional indexing.*")
-os.environ["WANDB_IGNORE_GLOBS"] = "*.pt;*.pth"
-
-def set_seed(seed=42):
-    print(f"[DEBUG] ðŸŒ± Setting global seed to {seed}")
-    random.seed(seed)
-    np.random.seed(seed)
-    torch.manual_seed(seed)
-    torch.cuda.manual_seed_all(seed)
-    
-    # FOR TESTING & PRODUCTION TRAINING (SPEED):
-    torch.backends.cudnn.deterministic = False
-    torch.backends.cudnn.benchmark = True
-
-def cleanup_gpu():
-    gc.collect()
-    torch.cuda.empty_cache()
-
-def anatomix_normalize(tensor, percentile_range = None, clip_range=None):
-    if not torch.is_tensor(tensor):
-        tensor = torch.as_tensor(tensor, dtype=torch.float32)
-    
-    # 1. CT path: Explicit Clipping (Windowing)
-    if clip_range is not None:
-        min_c, max_c = clip_range
-        tensor = torch.clamp(tensor, min_c, max_c)
-        denom = max_c - min_c
-        if denom == 0:
-            print(f"[WARNING] CT Window has 0 width: {clip_range}")
-            return torch.zeros_like(tensor)
-        return (tensor - min_c) / denom
-    
-    # 2. MRI path: Percentile Normalization (Instance-level)
-    if percentile_range is not None:
-        min_percentile, max_percentile = percentile_range
-        v_min = torch.quantile(tensor.float(), min_percentile / 100.0)
-        v_max = torch.quantile(tensor.float(), max_percentile / 100.0)
-        tensor = torch.clamp(tensor, v_min, v_max)
-    
-        denom = v_max - v_min
-        if denom == 0:
-            print(f"[WARNING] MRI Volume is constant (Val: {v_min:.4f}). Returning zeros.")
-            return torch.zeros_like(tensor)
-            
-        return (tensor - v_min) / denom
-
-    # 3. just minmax normalization
-    v_min = tensor.min()
-    v_max = tensor.max()
-    # tensor = torch.clamp(tensor, v_min, v_max)
-    denom = v_max - v_min
-    if denom == 0:
-        print(f"[WARNING] MRI Volume is constant (Val: {v_min:.4f}). Returning zeros.")
-        return torch.zeros_like(tensor)
-        
-    return (tensor - v_min) / denom
-
-def unpad(data, original_shape, offset = 0):
-    if original_shape is None: return data
-    w_orig, h_orig, d_orig = original_shape
-    return data[..., offset:offset+w_orig, offset:offset+h_orig, offset:offset+d_orig]
-
-def compute_metrics(pred, target, data_range=1.0):
-    if pred.ndim != 5 or target.ndim != 5:
-        raise ValueError(f"Expected (B, C, D, H, W), got {pred.shape}")
-        
-    b, c, d, h, w = pred.shape
-    # NOTE: Reshaping for 2D SSIM calculation
-    pred_2d = pred.permute(0, 2, 1, 3, 4).reshape(-1, c, h, w)
-    targ_2d = target.permute(0, 2, 1, 3, 4).reshape(-1, c, h, w)
-    
-    ssim_val = fused_ssim(pred_2d, targ_2d, train=False).item()
-    
-    # PSNR
-    mse = torch.mean((pred - target) ** 2, dim=[1, 2, 3, 4])
-    mse = torch.clamp(mse, min=1e-10) 
-    psnr = 10 * torch.log10((data_range ** 2) / mse)
-
-    # 2. Gradient Difference (Sharpness Metric)
-    def get_gradients(img):
-        dz = torch.abs(img[:, :, 1:, :, :] - img[:, :, :-1, :, :])
-        dy = torch.abs(img[:, :, :, 1:, :] - img[:, :, :, :-1, :])
-        dx = torch.abs(img[:, :, :, :, 1:] - img[:, :, :, :, :-1])
-        return dz, dy, dx
-    pred_dz, pred_dy, pred_dx = get_gradients(pred)
-    targ_dz, targ_dy, targ_dx = get_gradients(target)
-    grad_diff = (
-        torch.mean(torch.abs(pred_dz - targ_dz)) +
-        torch.mean(torch.abs(pred_dy - targ_dy)) +
-        torch.mean(torch.abs(pred_dx - targ_dx))
-    ).item()
-
-    # 3. Bone Dice Coefficient (Structure Metric)
-    bone_thresh = 0.8
-    pred_bone = (pred > bone_thresh).float()
-    targ_bone = (target > bone_thresh).float()
-    intersection = (pred_bone * targ_bone).sum()
-    union = pred_bone.sum() + targ_bone.sum()
-    # Smooth Dice (Add epsilon to avoid division by zero)
-    dice_score = (2.0 * intersection + 1e-5) / (union + 1e-5)
-    dice_val = dice_score.item()
-    
-    return {
-        "mae": torch.mean(torch.abs(pred - target)).item(),
-        "psnr": torch.mean(psnr).item(),
-        "ssim": ssim_val,
-        "grad_diff": grad_diff, # ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ (Lower is better)
-        "bone_dice": dice_val,  # ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ (Higher is better)
-    }
-
-def get_subject_paths(root, subj_id):
-    data_path = os.path.join(root, "data")
-    ct = glob(os.path.join(data_path, subj_id, "ct_resampled.nii*"))
-    mr = glob(os.path.join(data_path, subj_id, "registration_output", "moved_*.nii*"))
-    if not ct or not mr: raise FileNotFoundError(f"Missing files for {subj_id}")
-    return {'ct': ct[0], 'mri': mr[0]}
-
-class Config(SimpleNamespace):
-    def __init__(self, dictionary, **kwargs):
-        super().__init__(**kwargs)
-        for key, value in dictionary.items():
-            setattr(self, key, value)
-
-# ==========================================
-# 1. MODELS & LOSS
-# ==========================================
-class CNNTranslator(nn.Module):
-    def __init__(self, in_channels=16, hidden_channels=32, depth=3, final_activation="relu_clamp", dropout=0.0):
-        super().__init__()
-        self.final_activation = final_activation
-        layers = []
-        
-        # Input
-        layers.append(nn.Conv3d(in_channels, hidden_channels, kernel_size=3, padding=1))
-        layers.append(nn.ReLU(inplace=True))
-        if dropout > 0: layers.append(nn.Dropout3d(p=dropout))
-        
-        # Hidden
-        for _ in range(depth - 2):
-            layers.append(nn.Conv3d(hidden_channels, hidden_channels, kernel_size=3, padding=1))
-            layers.append(nn.ReLU(inplace=True))
-            if dropout > 0: layers.append(nn.Dropout3d(p=dropout))
-        
-        # Output
-        layers.append(nn.Conv3d(hidden_channels, 1, kernel_size=3, padding=1))
-        self.net = nn.Sequential(*layers)
-
-    def forward(self, x):
-        x = self.net(x)
-        if self.final_activation == "sigmoid": return torch.sigmoid(x)
-        elif self.final_activation == "relu_clamp": return torch.clamp(torch.relu(x), 0, 1)
-        elif self.final_activation == "none": return x
-        else: raise ValueError(f"Unknown activation: {self.final_activation}")
-
-class CompositeLoss(nn.Module):
-    def __init__(self, weights):
-        super().__init__()
-        self.weights = weights
-        self.l1 = nn.L1Loss()
-        self.l2 = nn.MSELoss()
-
-    def forward(self, pred, target, feat_extractor=None, use_sliding_window=False):
-        total_loss = 0.0
-        loss_components = {}
-        
-        if self.weights.get("l1", 0) > 0:
-            val = self.l1(pred, target)
-            total_loss += self.weights["l1"] * val
-            loss_components["loss_l1"] = val.item()
-            
-        if self.weights.get("l2", 0) > 0:
-            val = self.l2(pred, target)
-            total_loss += self.weights["l2"] * val
-            loss_components["loss_l2"] = val.item()
-
-        if self.weights.get("ssim", 0) > 0:
-            b, c, d, h, w = pred.shape
-            pred_2d = pred.permute(0, 2, 1, 3, 4).reshape(-1, c, h, w).float()
-            targ_2d = target.permute(0, 2, 1, 3, 4).reshape(-1, c, h, w).float()
-            val = 1.0 - fused_ssim(pred_2d, targ_2d, train=True)
-            total_loss += self.weights["ssim"] * val
-            loss_components["loss_ssim"] = val.item()
-
-        if self.weights.get("perceptual", 0) > 0:
-            if feat_extractor is None: 
-                raise ValueError("Feat extractor missing for perceptual loss")
-            if use_sliding_window:
-                print("Skipping perceptual loss calculation during validation. NOTE: val loss will differ from train loss.")
-            else:
-                pred_feats = feat_extractor(pred)
-                with torch.no_grad(): target_feats = feat_extractor(target)
-                val = self.l1(pred_feats, target_feats)
-                total_loss += self.weights["perceptual"] * val
-                loss_components["loss_perceptual"] = val.item()
-            
-        return total_loss, loss_components
-
-# ==========================================
-# 2. DATA PIPELINE
-# ==========================================
-class DataPreprocessing(tio.Transform):
-    def __init__(self, patch_size=96, enable_safety_padding=False, res_mult=32, **kwargs):        
-        super().__init__(**kwargs)
-        self.patch_size = patch_size
-        self.enable_safety_padding = enable_safety_padding
-        self.res_mult = res_mult
-
-    def apply_transform(self, subject):
-        subject['ct'].set_data(anatomix_normalize(subject['ct'].data, clip_range=(-450, 450)).float())
-        subject['mri'].set_data(anatomix_normalize(subject['mri'].data, percentile_range=(0,99.99)).float())
-        # subject['mri'].set_data(anatomix_normalize(subject['mri'].data).float())
-        
-        # Save original shape
-        subject['original_shape'] = torch.tensor(subject['ct'].spatial_shape)
-        
-        pad_offset=0
-        # Padding logic
-        if self.enable_safety_padding:
-            pad_val = self.patch_size//2
-            subject = tio.Pad(pad_val, padding_mode=0)(subject)
-            pad_offset=pad_val
-
-        subject['pad_offset'] = pad_offset
-
-        current_shape = subject['ct'].spatial_shape
-        padding_params = []
-        for dim in current_shape:
-            target = max(self.patch_size, (int(dim) + self.res_mult - 1) // self.res_mult * self.res_mult)
-            padding_params.extend([0, int(target - dim)])
-            
-        if any(p > 0 for p in padding_params):
-            subject = tio.Pad(padding_params, padding_mode=0)(subject)
-            
-        # Probability Map for Sampler
-        if 'prob_map' not in subject:
-            prob = (subject['ct'].data > 0.01).to(torch.float32)
-            subject.add_image(tio.LabelMap(tensor=prob, affine=subject['mri'].affine), 'prob_map')
-
-        spatial_shape = subject['mri'].spatial_shape
-        if any(d % self.res_mult != 0 for d in spatial_shape):
-             print(f"[WARNING] Volume shape {spatial_shape} is not a multiple of {self.res_mult}!")
-            
-        return subject
-
-# def get_augmentations():
-#     return tio.Compose([
-#         tio.Compose([
-#             tio.RandomFlip(axes=(0, 1, 2), p=0.5),
-#             tio.RandomAffine(scales=(0.9, 1.1), degrees=10, translation=5, p=0.5),
-#             tio.RandomElasticDeformation(num_control_points=7, max_displacement=7, p=0.25),
-#         ]), # Geometric (Both)
-#         tio.Compose([
-#             tio.RandomBiasField(p=0.5, include=['mri']), 
-#             tio.RandomNoise(std=0.02, p=0.25, include=['mri']),
-#             tio.RandomGamma(log_gamma=(-0.3, 0.3), p=0.5, include=['mri'])
-#         ])  # Intensity (MRI only)
-#     ])
-
-def get_augmentations():
-    return tio.Compose([
-        # Applied to BOTH MRI and CT identically
-        tio.OneOf({
-            tio.RandomElasticDeformation(
-                num_control_points=7, 
-                max_displacement=4, 
-                locked_borders=2, 
-                image_interpolation='bspline' 
-            ): 0.3, 
-            tio.RandomAffine(
-                scales=(0.95, 1.1), 
-                degrees=7, 
-                translation=4,
-                default_pad_value='minimum'
-            ): 0.7,
-        }, p=0.8), 
-        tio.RandomFlip(axes=(0, 1, 2), p=0.5),
-        tio.Clamp(0, 1),
-        
-        tio.Compose([
-            tio.RandomBiasField(coefficients=0.3, p=0.4),
-            tio.RandomGamma(log_gamma=(-0.2, 0.2), p=0.4),
-        ], include=['mri']) ,
-        tio.Clamp(0, 1),
-    ])
-    
-# ==========================================
-# 3. TRAINER CLASS
-# ==========================================
-class Trainer:
-    def __init__(self, config_dict):
-        # 1. Config Setup
-        self.cfg = Config(config_dict)
-        set_seed(self.cfg.seed)
-        self.device = torch.device(self.cfg.device)
-        print(f"[DEBUG] ðŸš€ Initializing Trainer on {self.device}")
-
-        # 2. Setup Components
-        self._setup_models()
-        self._setup_data()
-        self._setup_opt()
-        self._setup_wandb()
-        
-        # 3. State Tracking
-        self.start_epoch = 0
-        self.global_start_time = None 
-        self._load_resume()
-
-    def _setup_wandb(self):
-        if not self.cfg.wandb: return
-        
-        run_name = f"{self.cfg.model_type.upper()}_Train{len(self.train_subjects)}"
-        os.makedirs(self.cfg.log_dir, exist_ok=True)
-        
-        print(f"[DEBUG] ðŸ“¡ Initializing WandB: {run_name}")
-        wandb.init(
-            project=self.cfg.project_name, 
-            name=run_name, 
-            config=vars(self.cfg),
-            notes=self.cfg.wandb_note,
-            reinit=True,
-            dir=self.cfg.log_dir,
-            id=self.cfg.resume_wandb_id, 
-            resume="allow",
-        )
-        if not self.cfg.resume_wandb_id:
-            wandb.run.log_code(root=self.cfg.root_dir, include_fn=lambda path: path.endswith(".py"))
-
-    def _setup_data(self):
-        # 1. Discovery
-        data_dir = os.path.join(self.cfg.root_dir, "data")
-        candidates = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])
-        
-        if self.cfg.subjects:
-            candidates = self.cfg.subjects
-        elif self.cfg.region:
-            candidates = [c for c in candidates if self.cfg.region.upper() in c]
-            
-        # Validate existence
-        valid_subjs = []
-        for s in candidates:
-            if os.path.exists(os.path.join(data_dir, s, "ct_resampled.nii.gz")):
-                valid_subjs.append(s)
-        
-        # 2. Split
-        random.shuffle(valid_subjs)
-        num_val = max(1, int(len(valid_subjs) * self.cfg.val_split))
-        self.train_subjects = valid_subjs[:-num_val]
-        self.val_subjects = valid_subjs[-num_val:]
-        
-        print(f"[DEBUG] ðŸ“Š Data Split - Train: {len(self.train_subjects)} | Val: {len(self.val_subjects)}")
-
-        if self.cfg.analyze_shapes:
-            shapes = []
-            for s in tqdm(self.train_subjects, desc="Analyzing Shapes"):
-                try:
-                    p = get_subject_paths(self.cfg.root_dir, s)
-                    sh = nib.load(p['mri']).header.get_data_shape()
-                    shapes.append(sh)
-                except Exception: pass
-            
-            if shapes:
-                avg_shape = np.mean(np.array(shapes), axis=0).astype(int)
-                print(f"ðŸ“Š Mean Volume Shape: {tuple(int(x) for x in avg_shape)}")
-                if np.any(avg_shape < self.cfg.patch_size):
-                     print(f"âš ï¸ Warning: Mean shape {tuple(avg_shape)} is smaller than patch size {self.cfg.patch_size} in some dims. (Auto-padding is active to prevent crashes)")
-        
-        # 3. Helper to create paths
-        def _make_subj_list(subjs):
-            return [tio.Subject(
-                mri=tio.ScalarImage(p['mri']), 
-                ct=tio.ScalarImage(p['ct']),
-                subj_id=s
-            ) for s in subjs for p in [get_subject_paths(self.cfg.root_dir, s)]]
-
-        # 5. Train Loader (Queue)
-        train_objs = _make_subj_list(self.train_subjects)
-        use_safety = (self.cfg.model_type.lower() == "cnn" and self.cfg.enable_safety_padding)
-        
-        preprocess = DataPreprocessing(patch_size=self.cfg.patch_size, enable_safety_padding=use_safety, res_mult=self.cfg.res_mult)
-        transforms = tio.Compose([preprocess, get_augmentations()]) if self.cfg.augment else preprocess
-        
-        train_ds = tio.SubjectsDataset(train_objs, transform=transforms)
-        sampler = tio.WeightedSampler(patch_size=self.cfg.patch_size, probability_map='prob_map')
-        
-        queue = tio.Queue(
-            subjects_dataset=train_ds,
-            samples_per_volume=self.cfg.patches_per_volume,
-            max_length=max(self.cfg.patches_per_volume, self.cfg.data_queue_max_length),
-            sampler=sampler,
-            num_workers=self.cfg.data_queue_num_workers,
-            shuffle_patches=True,
-            shuffle_subjects=True
-        )
-        self.train_loader = tio.SubjectsLoader(queue, batch_size=self.cfg.batch_size, num_workers=0)
-        
-        # Create infinite iterator
-        def _inf_gen(loader):
-            while True:
-                for batch in loader: yield batch
-        self.train_iter = _inf_gen(self.train_loader)
-
-        # 6. Val Loader (Full Volume)
-        val_objs = _make_subj_list(self.val_subjects)
-        val_preprocess = DataPreprocessing(patch_size=self.cfg.patch_size, enable_safety_padding=False, res_mult=self.cfg.res_mult)
-        val_ds = tio.SubjectsDataset(val_objs, transform=val_preprocess) 
-        self.val_loader = torch.utils.data.DataLoader(val_ds, batch_size=1, shuffle=False)
-
-    def _setup_models(self):
-        # 1. Anatomix (Feature Extractor)
-        print(f"[DEBUG] ðŸ—ï¸ Building Anatomix ({self.cfg.anatomix_weights})...")
-        if self.cfg.anatomix_weights == "v1":
-            self.cfg.res_mult = 16 
-            self.feat_extractor = Unet(3, 1, 16, 4, 16).to(self.device)
-            ckpt = os.path.join(self.cfg.root_dir, "anatomix", "model-weights", "anatomix.pth")
-        elif self.cfg.anatomix_weights == "v2":
-            self.cfg.res_mult = 32
-            self.feat_extractor = Unet(3, 1, 16, 5, 20, norm="instance", interp="trilinear", pooling="Avg").to(self.device)
-            # Optimize inference speed
-            self.feat_extractor = torch.compile(self.feat_extractor, mode="default")
-            ckpt = os.path.join(self.cfg.root_dir, "anatomix", "model-weights", "best_val_net_G.pth")
-        else:
-            raise ValueError("Invalid anatomix_weights")
-            
-        if os.path.exists(ckpt):
-            self.feat_extractor.load_state_dict(torch.load(ckpt, map_location=self.device), strict=True)
-            print(f"[DEBUG] Loaded Anatomix weights from {ckpt}")
-        else:
-            print(f"[WARNING] âš ï¸ Anatomix weights NOT FOUND at {ckpt}")
-
-        if not self.cfg.finetune_feat_extractor:
-            for p in self.feat_extractor.parameters(): p.requires_grad = False
-            self.feat_extractor.eval()
-        
-        # 2. CNN Translator
-        print(f"[DEBUG] ðŸ—ï¸ Building CNN (D={self.cfg.cnn_depth}, H={self.cfg.cnn_hidden})...")
-        model = CNNTranslator(
-            in_channels=16,
-            hidden_channels=self.cfg.cnn_hidden,
-            depth=self.cfg.cnn_depth,
-            final_activation=self.cfg.final_activation,
-            dropout=self.cfg.dropout
-        ).to(self.device)
-        # self.model = torch.compile(model, mode="reduce-overhead")
-        # self.model = torch.compile(model, mode="default")
-        if self.cfg.model_compile_mode:
-            print(f"[DEBUG] ðŸš€ Compiling model with mode: {self.cfg.model_compile_mode}")
-            self.model = torch.compile(model, mode=self.cfg.model_compile_mode)
-        else:
-            print(f"[DEBUG] ðŸ¢ specific compile mode not set or None. Skipping compilation.")
-            self.model = model
-
-    def _setup_opt(self):
-        params = [{'params': self.model.parameters(), 'lr': self.cfg.lr}]
-        if self.cfg.finetune_feat_extractor:
-            params.append({'params': self.feat_extractor.parameters(), 'lr': self.cfg.lr_feat_extractor})
-            
-        self.optimizer = torch.optim.Adam(params)
-        
-        # Auto-Pilot Scheduler
-        # patience=10: Waits 10 validation checks (10 * 5 = 50 epochs) before dropping.
-        # factor=0.5: When stuck, cuts LR in half.
-        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
-            self.optimizer, 
-            mode='min', 
-            factor=0.5, 
-            patience=10, 
-            min_lr=1e-6,
-        )
-
-        self.loss_fn = CompositeLoss(weights={
-            "l1": self.cfg.l1_w, "l2": self.cfg.l2_w, 
-            "ssim": self.cfg.ssim_w, "perceptual": self.cfg.perceptual_w
-        }).to(self.device)
-        self.scaler = torch.cuda.amp.GradScaler()
-        
-    def _load_resume(self):
-        if not self.cfg.resume_wandb_id: return
-        
-        print(f"[RESUME] ðŸ•µï¸ Searching for Run ID: {self.cfg.resume_wandb_id}")
-        run_folders = glob(os.path.join(self.cfg.log_dir, "wandb", f"run-*-{self.cfg.resume_wandb_id}"))
-        if not run_folders:
-            print("[RESUME] âŒ Run folder not found.")
-            return
-
-        all_ckpts = []
-        for f in run_folders:
-            ckpts = glob(os.path.join(f, "files", "*.pt"))
-            all_ckpts.extend(ckpts)
-            
-        if not all_ckpts:
-            print("[RESUME] âš ï¸ No checkpoints found inside run folder.")
-            return
-
-        resume_path = max(all_ckpts, key=os.path.getmtime)
-        print(f"[RESUME] ðŸ“¥ Loading: {resume_path}")
-        checkpoint = torch.load(resume_path, map_location=self.device)
-        
-        self.model.load_state_dict(checkpoint['model_state_dict'])
-        if 'optimizer_state_dict' in checkpoint:
-            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
-        if 'epoch' in checkpoint:
-            self.start_epoch = checkpoint['epoch'] + 1
-    
-    def save_checkpoint(self, epoch, is_final=False):
-        filename = f"{self.cfg.model_type}_{'FINAL' if is_final else f'epoch{epoch:05d}_{datetime.datetime.now():%Y%m%d_%H%M}'}.pt"
-        save_dir = wandb.run.dir if self.cfg.wandb else os.path.join(self.cfg.root_dir, "results", "models")
-        os.makedirs(save_dir, exist_ok=True)
-        
-        save_dict = {
-            'epoch': epoch,
-            'model_state_dict': self.model.state_dict(),
-            'optimizer_state_dict': self.optimizer.state_dict(),
-            'config': vars(self.cfg)
-        }
-        
-        if self.cfg.finetune_feat_extractor:
-             save_dict['feat_extractor_state_dict'] = self.feat_extractor.state_dict()
-             
-        path = os.path.join(save_dir, filename)
-        torch.save(save_dict, path)
-        print(f"[SAVE] ðŸ’¾ Checkpoint saved: {path}")
-        if self.cfg.wandb:
-            wandb.log({"info/checkpoint_path": path}, commit=False)
-            
-    # ==========================================
-    # VISUALIZATION
-    # ==========================================
-    def _log_aug_viz(self, epoch):
-        try:
-            # 1. Get Data
-            subj_id = self.val_subjects[0] 
-            paths = get_subject_paths(self.cfg.root_dir, subj_id)
-            
-            subj = tio.Subject(mri=tio.ScalarImage(paths['mri']), ct=tio.ScalarImage(paths['ct']))
-            prep = DataPreprocessing(patch_size=self.cfg.patch_size, res_mult=self.cfg.res_mult)
-            subj = prep(subj)
-
-            # 2. Augment
-            aug = get_augmentations()(subj)
-            hist_str = " | ".join([t.name for t in aug.history])
-
-            # 3. Slice & Plot
-            z = subj['mri'].shape[-1] // 2
-            
-            # NOTE: If aug changes shape, this line will crash.
-            orig_sl = subj['mri'].data[0, ..., z].numpy()
-            aug_sl = aug['mri'].data[0, ..., z].numpy()
-
-            fig, ax = plt.subplots(1, 3, figsize=(10, 4))
-            ax[0].imshow(np.rot90(orig_sl), cmap='gray', vmin=0, vmax=1); ax[0].set_title(f"Original")
-            ax[1].imshow(np.rot90(aug_sl), cmap='gray', vmin=0, vmax=1); ax[1].set_title(f"Augmented\n{hist_str}")
-            
-            # The simple Diff you wanted
-            ax[2].imshow(np.rot90(aug_sl - orig_sl), cmap='seismic', vmin=-0.5, vmax=0.5); ax[2].set_title("Diff")
-            
-            wandb.log({"val/aug_viz": wandb.Image(fig)}, step=epoch)
-            plt.close(fig)
-        except Exception as e:
-            print(f"[WARNING] Aug Viz failed: {e}")
-
-    
-    @torch.no_grad()
-    def _visualize_full(self, pred, ct, mri, feats_mri, subj_id, shape, epoch, idx, offset=0):
-        """
-        Full 8-column visualization with PCA, Cosine Sim, and Residuals.
-        """
-        # 1. Extract Features for Comparison
-        def extract_np(vol_tensor):
-            inp = vol_tensor.to(self.device)
-            if inp.ndim == 4: inp = inp.unsqueeze(0) # Handle missing batch dim
-            return self.feat_extractor(inp).squeeze(0).cpu().numpy()
-
-        feats_gt = extract_np(ct)
-        feats_pred = extract_np(pred)
-        # feats_mri is already extracted, just convert to numpy
-        feats_mri_np = feats_mri.squeeze(0).cpu().numpy()
-        
-        # 2. Unpad Volumes
-        w, h, d = shape
-        gt_ct = unpad(ct, shape, offset).cpu().numpy().squeeze()
-        gt_mri = unpad(mri, shape, offset).cpu().numpy().squeeze()
-        pred_ct = unpad(pred, shape, offset).cpu().numpy().squeeze()
-        
-        # 3. Unpad Features (C, W, H, D)
-        feats_gt = feats_gt[..., offset:offset+w, offset:offset+h, offset:offset+d]
-        feats_pred = feats_pred[..., offset:offset+w, offset:offset+h, offset:offset+d]
-        feats_mri_np = feats_mri_np[..., offset:offset+w, offset:offset+h, offset:offset+d]
-
-        C, H, W, D_dim = feats_gt.shape
-        
-        # 4. Define Items
-        items = [
-            (gt_mri, "GT MRI", "gray", (0,1)),
-            (gt_ct, "GT CT", "gray", (0,1)),
-            (pred_ct, "Pred CT", "gray", (0,1)),
-            (pred_ct - gt_ct, "Residual", "seismic", (-0.5, 0.5)),
-        ]
-
-        # 5. PCA Logic
-        if self.cfg.viz_pca:
-            def sample_vox(f, max_v=200_000):
-                X = f.reshape(C, -1).T
-                if X.shape[0] > max_v: X = X[np.random.choice(X.shape[0], max_v, replace=False)]
-                return X
-            
-            X_all = np.concatenate([sample_vox(feats_mri_np), sample_vox(feats_gt), sample_vox(feats_pred)], axis=0)
-            pca = PCA(n_components=3, svd_solver="randomized").fit(X_all)
-            
-            def proj(f):
-                Y = pca.transform(f.reshape(C, -1).T)
-                Y = (Y - Y.min(0, keepdims=True)) / (Y.max(0, keepdims=True) - Y.min(0, keepdims=True) + 1e-8)
-                return Y.reshape(H, W, D_dim, 3)
-
-            items.extend([
-                (proj(feats_mri_np), "PCA (MRI)", None, None),
-                (proj(feats_gt), "PCA (GT CT)", None, None),
-                (proj(feats_pred), "PCA (Pred)", None, None),
-            ])
-
-        # 6. Cosine Similarity
-        gt_t = torch.from_numpy(feats_gt).unsqueeze(0)
-        pred_t = torch.from_numpy(feats_pred).unsqueeze(0)
-        cos_sim = F.cosine_similarity(gt_t, pred_t, dim=1).squeeze(0).numpy()
-        cos_sim_n = (cos_sim - cos_sim.min()) / (cos_sim.max() - cos_sim.min() + 1e-8)
-        items.append((cos_sim_n, "Cosine Sim", "plasma", (0,1)))
-
-        # 7. Plotting
-        num_cols = len(items)
-        slice_indices = np.linspace(0.1 * D_dim, 0.9 * D_dim, 5, dtype=int)
-        
-        fig, axes = plt.subplots(len(slice_indices), num_cols, figsize=(4 * num_cols, 3.5 * len(slice_indices)))
-        plt.subplots_adjust(wspace=0.05, hspace=0.15)
-        
-        # Handle single row edge case
-        if len(slice_indices) == 1: axes = axes.reshape(1, -1)
-
-        for i, z_slice in enumerate(slice_indices):
-            for j, (data, title, cmap, clim) in enumerate(items):
-                ax = axes[i, j]
-                if data.ndim == 3: # (H, W, D)
-                    im = ax.imshow(data[:, :, z_slice], cmap=cmap, vmin=clim[0], vmax=clim[1])
-                    if title == "Residual": res_im = im
-                    if title == "Cosine Sim": cos_im = im
-                else: # (H, W, D, 3) RGB
-                    ax.imshow(data[:, :, z_slice, :])
-                
-                if i == 0: ax.set_title(title)
-                ax.axis("off")
-
-        # Colorbars
-        if 'res_im' in locals():
-            cbar = fig.colorbar(res_im, ax=axes[:, 3], fraction=0.04, pad=0.01)
-            cbar.set_label("Residual Error")
-        
-        cbar2 = fig.colorbar(cos_im, ax=axes[:, num_cols-1], fraction=0.04, pad=0.01)
-        cbar2.set_label("Cosine Similarity")
-
-        fig.suptitle(f"Viz â€” {subj_id} (Ep {epoch})", fontsize=16, y=0.99)
-        
-        if self.cfg.wandb:
-            wandb.log({f"viz/{'train' if idx==-1 else ('val_'+ str(idx))}": wandb.Image(fig)}, step=epoch)
-        plt.close(fig)
-
-    @torch.no_grad()
-    def _visualize_lite(self, pred, ct, mri, subj_id, shape, epoch, idx, offset=0):
-        """
-        Lightweight visualization: MRI, GT, Pred, Residual. 
-        """
-        # 1. Unpad Volumes
-        w, h, d = shape
-        gt_ct = unpad(ct, shape, offset).cpu().numpy().squeeze()
-        gt_mri = unpad(mri, shape, offset).cpu().numpy().squeeze()
-        pred_ct = unpad(pred, shape, offset).cpu().numpy().squeeze()
-        
-        # 2. Define Items (Standard 4-column view)
-        items = [
-            (gt_mri, "GT MRI", "gray", (0,1)),
-            (gt_ct, "GT CT", "gray", (0,1)),
-            (pred_ct, "Pred CT", "gray", (0,1)),
-            (pred_ct - gt_ct, "Residual", "seismic", (-0.5, 0.5)),
-        ]
-
-        # 3. Plotting
-        D_dim = gt_ct.shape[-1]
-        num_cols = len(items)
-        slice_indices = np.linspace(0.1 * D_dim, 0.9 * D_dim, 5, dtype=int)
-        
-        fig, axes = plt.subplots(len(slice_indices), num_cols, figsize=(3 * num_cols, 3.5 * len(slice_indices)))
-        plt.subplots_adjust(wspace=0.05, hspace=0.15)
-        
-        if len(slice_indices) == 1: axes = axes.reshape(1, -1)
-
-        for i, z_slice in enumerate(slice_indices):
-            for j, (data, title, cmap, clim) in enumerate(items):
-                ax = axes[i, j]
-                im = ax.imshow(data[:, :, z_slice], cmap=cmap, vmin=clim[0], vmax=clim[1])
-                
-                if title == "Residual": res_im = im
-                if i == 0: ax.set_title(title)
-                ax.axis("off")
-
-        if 'res_im' in locals():
-            cbar = fig.colorbar(res_im, ax=axes[:, 3], fraction=0.04, pad=0.01)
-            cbar.set_label("Residual Error")
-        
-        fig.suptitle(f"Viz LITE â€” {subj_id} (Ep {epoch})", fontsize=16, y=0.99)
-        
-        if self.cfg.wandb:
-            wandb.log({f"viz/{('val_'+ str(idx))}": wandb.Image(fig)}, step=epoch)
-        plt.close(fig)
-        
-    # Added method to visualize training patches
-    @torch.no_grad()
-    def _log_training_patch(self, mri, ct, pred, epoch, step):
-        """
-        Visualizes MRI, Prediction, and CT (GT) for the first patch in the batch.
-        """
-        # 1. Prepare Data (Batch 0, Channel 0)
-        img_in = mri[0, 0].detach().cpu().float().numpy()
-        img_gt = ct[0, 0].detach().cpu().float().numpy()
-        img_pred = pred[0, 0].detach().cpu().float().numpy()
-        
-        # Center indices
-        cx, cy, cz = np.array(img_in.shape) // 2
-
-        # 3 Rows (MRI, Pred, CT), 3 Cols (Axial, Sagittal, Coronal)
-        fig, axes = plt.subplots(3, 3, figsize=(10, 10))
-        
-        # Helper to plot a row
-        def plot_row(row_idx, vol, title_prefix, vmin=None, vmax=None):
-            # Axial
-            axes[row_idx, 0].imshow(np.rot90(vol[:, :, cz]), cmap='gray', vmin=vmin, vmax=vmax)
-            axes[row_idx, 0].set_title(f"{title_prefix} Ax")
-            # Sagittal
-            axes[row_idx, 1].imshow(np.rot90(vol[cx, :, :]), cmap='gray', vmin=vmin, vmax=vmax)
-            axes[row_idx, 1].set_title(f"{title_prefix} Sag")
-            # Coronal
-            axes[row_idx, 2].imshow(np.rot90(vol[:, cy, :]), cmap='gray', vmin=vmin, vmax=vmax)
-            axes[row_idx, 2].set_title(f"{title_prefix} Cor")
-            
-            # Add stats text to the left of the row
-            axes[row_idx, 0].text(-5, 10, f"Min: {vol.min():.2f}\nMax: {vol.max():.2f}", 
-                                  fontsize=8, color='white', backgroundcolor='black')
-
-        # Row 1: MRI (Auto-scaled intensity)
-        plot_row(0, img_in, "MRI")
-
-        # Row 2: Prediction (Fixed 0-1 range to match CT)
-        plot_row(1, img_pred, "Pred", vmin=0, vmax=1)
-
-        # Row 3: CT (Fixed 0-1 range)
-        plot_row(2, img_gt, "GT CT", vmin=0, vmax=1)
-
-        # Cleanup
-        for ax in axes.flatten():
-            ax.axis('off')
-        
-        plt.tight_layout()
-        
-        wandb.log({f"train/patch_viz": wandb.Image(fig)}, step=epoch)
-        plt.close(fig)
-        
-    # ==========================================
-    # CORE LOGIC
-    # ==========================================
-    def train_epoch(self, epoch):
-        self.model.train()
-        if self.cfg.finetune_feat_extractor: self.feat_extractor.train()
-        else: self.feat_extractor.eval()
-        
-        total_loss = 0.0
-        total_grad = 0.0
-        comp_accum = {}
-        
-        pbar = tqdm(range(self.cfg.steps_per_epoch), desc=f"Train Ep {epoch}", leave=False, dynamic_ncols=True)
-        
-        for step_idx in pbar:
-            self.optimizer.zero_grad(set_to_none=True)
-            step_loss = 0.0
-            
-            for _ in range(self.cfg.accum_steps):
-                batch = next(self.train_iter)
-                mri = batch['mri'][tio.DATA].to(self.device, non_blocking=True)
-                ct = batch['ct'][tio.DATA].to(self.device, non_blocking=True)
-
-
-                # Use 'dtype=torch.bfloat16' for Ampere+ GPUs (3090, 4090, A100, A6000)
-                # Use 'dtype=torch.float16' for older GPUs (2080, V100, Titan)
-                with torch.autocast(device_type="cuda", dtype=torch.bfloat16):
-                    if self.cfg.finetune_feat_extractor:
-                        features = self.feat_extractor(mri)
-                    else:
-                        with torch.no_grad(): features = self.feat_extractor(mri)
-                    
-                    pred = self.model(features)
-                    
-                    if self.cfg.wandb and step_idx == 0:
-                        self._log_training_patch(mri, ct, pred, epoch, step_idx)
-                    
-                    fe_ref = self.feat_extractor if self.cfg.perceptual_w > 0 else None
-                    loss, comps = self.loss_fn(pred, ct, feat_extractor=fe_ref)
-                    
-                    for k, v in comps.items():
-                        comp_accum[k] = comp_accum.get(k, 0.0) + (v / self.cfg.accum_steps)
-                    
-                    loss = loss / self.cfg.accum_steps
-                    self.scaler.scale(loss).backward()
-                    step_loss += loss.item()
-
-            self.scaler.unscale_(self.optimizer)
-            grad_norm = torch.nn.utils.clip_grad_norm_(
-                list(self.model.parameters()) + (list(self.feat_extractor.parameters()) if self.cfg.finetune_feat_extractor else []), 
-                max_norm=1.0
-            )
-            self.scaler.step(self.optimizer)
-            self.scaler.update()
-            
-            total_loss += step_loss
-            total_grad += grad_norm.item()
-            pbar.set_postfix({"loss": f"{step_loss:.4f}", "gn": f"{grad_norm.item():.2f}"})
-            
-        return total_loss / self.cfg.steps_per_epoch, \
-               {k: v / self.cfg.steps_per_epoch for k, v in comp_accum.items()}, \
-               total_grad / self.cfg.steps_per_epoch
-
-    @torch.no_grad()
-    def validate(self, epoch):
-        self.model.eval()
-        val_metrics = defaultdict(list)
-        
-        # 1. Validation Loop
-        for i, batch in enumerate(tqdm(self.val_loader, desc="Validating", leave=False)):
-            mri = batch['mri'][tio.DATA].to(self.device)
-            ct = batch['ct'][tio.DATA].to(self.device)
-            orig_shape = batch['original_shape'][0].tolist()
-            subj_id = batch['subj_id'][0]
-            pad_offset = batch['pad_offset'][0].item() if 'pad_offset' in batch else 0
-            
-            # Sliding Window (Lite) vs Full Volume (Standard)
-            feats = None
-            if self.cfg.val_sliding_window:
-                def combined_forward(x):
-                    return self.model(self.feat_extractor(x))
-
-                pred = sliding_window_inference(
-                    inputs=mri, 
-                    roi_size=(self.cfg.patch_size, self.cfg.patch_size, self.cfg.patch_size), 
-                    sw_batch_size=self.cfg.val_sw_batch_size, 
-                    predictor=combined_forward,
-                    overlap=self.cfg.val_sw_overlap,
-                    mode="gaussian",
-                    device=self.device
-                )
-            else:
-                feats = self.feat_extractor(mri)
-                pred = self.model(feats)
-            
-            # Metrics
-            pred_unpad = unpad(pred, orig_shape, pad_offset)
-            ct_unpad = unpad(ct, orig_shape, pad_offset)
-            met = compute_metrics(pred_unpad, ct_unpad)
-            
-            # Loss (Composite)
-            l_val, _ = self.loss_fn(pred, ct, feat_extractor=self.feat_extractor, use_sliding_window = self.cfg.val_sliding_window)
-            met['loss'] = l_val.item()
-            
-            for k, v in met.items():
-                val_metrics[k].append(v)
-            
-            # Viz
-            if self.cfg.wandb and i < self.cfg.viz_limit:
-                if self.cfg.val_sliding_window:
-                    self._visualize_lite(pred, ct, mri, subj_id, orig_shape, epoch, idx=i, offset=pad_offset)
-                else:
-                    self._visualize_full(pred, ct, mri, feats, subj_id, orig_shape, epoch, idx=i, offset=pad_offset)
-        
-        # 2. Augmentation Viz (Updated)
-        if self.cfg.wandb and self.cfg.augment:
-             self._log_aug_viz(epoch)
-
-        # 3. Log
-        avg_met = {k: np.mean(v) for k, v in val_metrics.items()}
-        if self.cfg.wandb:
-            wandb.log({f"val/{k}": v for k, v in avg_met.items()}, step=epoch)
-
-        return avg_met
-
-    def train(self):
-        print(f"[DEBUG] ðŸ Starting Loop: Ep {self.start_epoch} -> {self.cfg.total_epochs}")
-        self.global_start_time = time.time()
-        
-        if self.cfg.sanity_check and not self.cfg.resume_wandb_id:
-            print("[DEBUG] running sanity check...")
-            avg_met = self.validate(0)
-            tqdm.write(
-                    f"Ep -1 | Train: 0.0000 | Val: {avg_met.get('loss',0):.4f} | "
-                    f"SSIM: {avg_met.get('ssim',0):.4f} | PSNR: {avg_met.get('psnr',0):.2f}"
-                )
-
-        global_pbar = tqdm(
-            range(self.start_epoch, self.cfg.total_epochs),
-            desc="ðŸš€ Total Progress",
-            initial=self.start_epoch,
-            total=self.cfg.total_epochs,
-            dynamic_ncols=True,
-            unit="ep"
-        )
-            
-        for epoch in global_pbar:
-            ep_start = time.time()
-            
-            loss, comps, gn = self.train_epoch(epoch)
-            
-            if epoch % self.cfg.val_interval == 0 or (epoch+1) == self.cfg.total_epochs:
-                avg_met = self.validate(epoch)
-                val_loss = avg_met.get('loss', 0)
-
-                self.scheduler.step(val_loss)
-                
-                tqdm.write(
-                    f"Ep {epoch} | Train: {loss:.4f} | Val: {val_loss:.4f} | "
-                    f"SSIM: {avg_met.get('ssim',0):.4f} | PSNR: {avg_met.get('psnr',0):.2f}"
-                )
-
-            ep_duration = time.time() - ep_start
-            cumulative_time = time.time() - self.global_start_time
-
-            if self.cfg.wandb:
-                current_lr = self.optimizer.param_groups[0]['lr']
-                log = {
-                    "train_loss/total": loss, 
-                    "info/grad_norm": gn, 
-                    "info/epoch_duration": ep_duration,
-                    "info/cumulative_time": cumulative_time,
-                    "info/lr": current_lr 
-                }
-                for k, v in comps.items(): log[k.replace("loss_", "train_loss/")] = v
-                wandb.log(log, step=epoch)
-                
-            if epoch % self.cfg.model_save_interval == 0:
-                self.save_checkpoint(epoch)
-                
-        self.save_checkpoint(self.cfg.total_epochs, is_final=True)
-        if self.cfg.wandb: wandb.finish()
-        print(f"âœ… Training Complete. Total Time: {time.time() - self.global_start_time:.1f}s")
-
-# ==========================================
-# 4. CONFIG & EXECUTION
-# ==========================================
-DEFAULT_CONFIG = {
-    # System
-    "root_dir": "/home/minsukc/MRI2CT",
-    "log_dir": "/gpfs/accounts/jjparkcv_root/jjparkcv98/minsukc/MRI2CT/wandb_logs",
-    "seed": 42,
-    "device": "cuda" if torch.cuda.is_available() else "cpu",
-    "wandb": True,
-    "project_name": "mri2ct",
-    
-    # Data
-    "subjects": None,
-    "region": None, # "AB", "TH", "HN"
-    "val_split": 0.1,
-    "augment": True,
-    "patch_size": 96,
-    "patches_per_volume": 30,
-    "data_queue_max_length": 500,
-    "data_queue_num_workers": 4,
-    "anatomix_weights": "v2", # "v1", "v2"
-    "res_mult": 32 ,
-    "analyze_shapes": True, 
-    
-    # Training
-    "lr": 3e-4,
-    "val_interval": 1,
-    "sanity_check": True,
-    "accum_steps": 2,
-    "model_save_interval": 50,
-    "viz_limit": 6,
-    "viz_pca": False,
-    "steps_per_epoch": 25,
-    "finetune_feat_extractor": False,
-    "lr_feat_extractor": 1e-5,
-    
-    # Model Choice
-    "model_type": "cnn",
-    "model_compile_mode": None, # "default", "reduce-overhead", None
-    "total_epochs": 5001,
-    "dropout": 0,
-    
-    # CNN Specifics
-    "batch_size": 4,
-    "cnn_depth": 9,
-    "cnn_hidden": 128,
-    "final_activation": "sigmoid",
-    "enable_safety_padding": True,
-
-    # Sliding Window & Viz Options
-    "val_sliding_window": True, 
-    "val_sw_batch_size": 4, 
-    "val_sw_overlap": 0.25, 
-    
-    # Loss Weights
-    "l1_w": 1.0,
-    "l2_w": 0.0,
-    "ssim_w": 1.0,
-    "perceptual_w": 0.0,
-
-    "wandb_note": "test_run",
-    "resume_wandb_id": None, 
-}
-
-EXPERIMENT_CONFIG = [
-    {
-        "total_epochs": 5000,
-        # "anatomix_weights": "v1",
-        "sanity_check": False,
-        "accum_steps": 4,
-        "batch_size": 8,
-        "val_interval": 2,
-
-        "wandb_note": "test_run_long (new augmentation + val sliding window inference + lr scheduling)",
-        
-        # "resume_wandb_id": "l2rpr7g5", 
-    },
-]
-
-if __name__ == "__main__":
-    print(f"ðŸ“š Found {len(EXPERIMENT_CONFIG)} experiments to run.")
-    
-    for i, exp in enumerate(EXPERIMENT_CONFIG):
-        print(f"\n{'='*40}")
-        print(f"STARTING EXPERIMENT {i+1}/{len(EXPERIMENT_CONFIG)}")
-        print(f"Config: {exp}")
-        print(f"{'='*40}\n")
-        
-        try:
-            # Merge Configs
-            conf = copy.deepcopy(DEFAULT_CONFIG)
-            conf.update(exp)
-            
-            # Execute
-            trainer = Trainer(conf)
-            trainer.train()
-            
-            # Clean up
-            del trainer
-            cleanup_gpu()
-            
-        except KeyboardInterrupt:
-            print("\nâ›” Interrupted by user.")
-            cleanup_gpu()
-            break
-        except Exception as e:
-            print(f"âŒ Experiment {i+1} Failed: {e}")
-            import traceback
-            traceback.print_exc()
\ No newline at end of file
diff --git a/src/train.py b/src/train.py
index e9558d2..74816a1 100644
--- a/src/train.py
+++ b/src/train.py
@@ -24,6 +24,7 @@ import nibabel as nib
 import wandb
 import torchio as tio
 from fused_ssim import fused_ssim
+from monai.inferers import sliding_window_inference
 
 from anatomix.model.network import Unet
 
@@ -35,6 +36,7 @@ torch.set_float32_matmul_precision('high')
 
 warnings.filterwarnings("ignore", category=UserWarning, module="monai.utils.module")
 warnings.filterwarnings("ignore", message=".*SubjectsLoader in PyTorch >= 2.3.*")
+warnings.filterwarnings("ignore", message=".*non-tuple sequence for multidimensional indexing.*")
 os.environ["WANDB_IGNORE_GLOBS"] = "*.pt;*.pth"
 
 def set_seed(seed=42):
@@ -144,12 +146,22 @@ def compute_metrics(pred, target, data_range=1.0):
         "bone_dice": dice_val,  # ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ (Higher is better)
     }
 
-def get_subject_paths(root, subj_id):
-    data_path = os.path.join(root, "data")
-    ct = glob(os.path.join(data_path, subj_id, "ct_resampled.nii*"))
-    mr = glob(os.path.join(data_path, subj_id, "registration_output", "moved_*.nii*"))
-    if not ct or not mr: raise FileNotFoundError(f"Missing files for {subj_id}")
-    return {'ct': ct[0], 'mri': mr[0]}
+def get_subject_paths(root, relative_path):
+    """
+    root: base directory (e.g., .../3.0x3.0x3.0mm)
+    relative_path: 'train/1ABA005' or just '1ABA005' if using flat structure
+    """
+    # Construct full path
+    subj_dir = os.path.join(root, relative_path)
+    
+    ct_path = os.path.join(subj_dir, "ct.nii.gz")
+    mr_path = os.path.join(subj_dir, "registration_output", "moved_mr.nii.gz")
+    
+    # Fallback for checking existence
+    if not os.path.exists(ct_path) or not os.path.exists(mr_path):
+        raise FileNotFoundError(f"Missing files in {subj_dir}")
+        
+    return {'ct': ct_path, 'mri': mr_path}
 
 class Config(SimpleNamespace):
     def __init__(self, dictionary, **kwargs):
@@ -195,7 +207,7 @@ class CompositeLoss(nn.Module):
         self.l1 = nn.L1Loss()
         self.l2 = nn.MSELoss()
 
-    def forward(self, pred, target, feat_extractor=None):
+    def forward(self, pred, target, feat_extractor=None, use_sliding_window=False):
         total_loss = 0.0
         loss_components = {}
         
@@ -220,11 +232,14 @@ class CompositeLoss(nn.Module):
         if self.weights.get("perceptual", 0) > 0:
             if feat_extractor is None: 
                 raise ValueError("Feat extractor missing for perceptual loss")
-            pred_feats = feat_extractor(pred)
-            with torch.no_grad(): target_feats = feat_extractor(target)
-            val = self.l1(pred_feats, target_feats)
-            total_loss += self.weights["perceptual"] * val
-            loss_components["loss_perceptual"] = val.item()
+            if use_sliding_window:
+                print("Skipping perceptual loss calculation during validation. NOTE: val loss will differ from train loss.")
+            else:
+                pred_feats = feat_extractor(pred)
+                with torch.no_grad(): target_feats = feat_extractor(target)
+                val = self.l1(pred_feats, target_feats)
+                total_loss += self.weights["perceptual"] * val
+                loss_components["loss_perceptual"] = val.item()
             
         return total_loss, loss_components
 
@@ -310,8 +325,8 @@ def get_augmentations():
         tio.Clamp(0, 1),
         
         tio.Compose([
-            tio.RandomBiasField(coefficients=0.3, p=0.4),
-            tio.RandomGamma(log_gamma=(-0.2, 0.2), p=0.4),
+            tio.RandomBiasField(coefficients=0.5, p=0.4),
+            tio.RandomGamma(log_gamma=(-0.3, 0.3), p=0.4),
         ], include=['mri']) ,
         tio.Clamp(0, 1),
     ])
@@ -356,35 +371,42 @@ class Trainer:
             resume="allow",
         )
         if not self.cfg.resume_wandb_id:
-            wandb.run.log_code(root=self.cfg.root_dir, include_fn=lambda path: path.endswith(".py"))
+            current_code_dir = os.path.dirname(os.path.abspath(__file__))
+            wandb.run.log_code(root=current_code_dir, include_fn=lambda path: path.endswith(".py"))
 
     def _setup_data(self):
-        # 1. Discovery
-        data_dir = os.path.join(self.cfg.root_dir, "data")
-        candidates = sorted([d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))])
+        print(f"[DEBUG] ðŸ“‚ Searching for data in: {self.cfg.root_dir}")
         
-        if self.cfg.subjects:
-            candidates = self.cfg.subjects
-        elif self.cfg.region:
-            candidates = [c for c in candidates if self.cfg.region.upper() in c]
-            
-        # Validate existence
-        valid_subjs = []
-        for s in candidates:
-            if os.path.exists(os.path.join(data_dir, s, "ct_resampled.nii.gz")):
-                valid_subjs.append(s)
+        # Helper to scan a folder
+        def scan_split(split_name):
+            split_dir = os.path.join(self.cfg.root_dir, split_name)
+            if not os.path.exists(split_dir): return []
+            return sorted([
+                os.path.join(split_name, d) # Store as relative path 'train/1ABA005'
+                for d in os.listdir(split_dir) 
+                if os.path.isdir(os.path.join(split_dir, d))
+            ])
+
+        train_candidates = scan_split("train")
+        val_candidates = scan_split("val")
         
-        # 2. Split
-        random.shuffle(valid_subjs)
-        num_val = max(1, int(len(valid_subjs) * self.cfg.val_split))
-        self.train_subjects = valid_subjs[:-num_val]
-        self.val_subjects = valid_subjs[-num_val:]
+        # Logic for 'subjects' (Single Image Optimization)
+        if self.cfg.subjects:
+            print(f"[DEBUG] ðŸŽ¯ Filtering specific subjects: {self.cfg.subjects}")
+            # Filter candidates that end with the requested ID
+            # e.g., if requested '1ABA005', match 'train/1ABA005'
+            self.train_subjects = [c for c in train_candidates + val_candidates if os.path.basename(c) in self.cfg.subjects]
+            self.val_subjects = self.train_subjects # Validate on the same subject for overfitting
+        else:
+            # Standard Mode: Use the existing splits
+            self.train_subjects = train_candidates
+            self.val_subjects = val_candidates
         
         print(f"[DEBUG] ðŸ“Š Data Split - Train: {len(self.train_subjects)} | Val: {len(self.val_subjects)}")
 
         if self.cfg.analyze_shapes:
             shapes = []
-            for s in tqdm(self.train_subjects, desc="Analyzing Shapes"):
+            for s in tqdm(self.train_subjects[:30], desc="Analyzing Shapes (Sample)"):
                 try:
                     p = get_subject_paths(self.cfg.root_dir, s)
                     sh = nib.load(p['mri']).header.get_data_shape()
@@ -394,35 +416,15 @@ class Trainer:
             if shapes:
                 avg_shape = np.mean(np.array(shapes), axis=0).astype(int)
                 print(f"ðŸ“Š Mean Volume Shape: {tuple(int(x) for x in avg_shape)}")
-                if np.any(avg_shape < self.cfg.patch_size):
-                     print(f"âš ï¸ Warning: Mean shape {tuple(avg_shape)} is smaller than patch size {self.cfg.patch_size} in some dims. (Auto-padding is active to prevent crashes)")
         
         # 3. Helper to create paths
         def _make_subj_list(subjs):
             return [tio.Subject(
                 mri=tio.ScalarImage(p['mri']), 
                 ct=tio.ScalarImage(p['ct']),
-                subj_id=s
+                subj_id=os.path.basename(s) # Extract just ID for logging
             ) for s in subjs for p in [get_subject_paths(self.cfg.root_dir, s)]]
 
-        # 4. Monitor Data (One subject for viz)
-        self.train_monitor_data = None
-        if self.train_subjects:
-            ref_subj = self.train_subjects[0]
-            paths = get_subject_paths(self.cfg.root_dir, ref_subj)
-            # Create a localized subject for viz
-            subj_viz = tio.Subject(mri=tio.ScalarImage(paths['mri']), ct=tio.ScalarImage(paths['ct']), subj_id=ref_subj)
-            # Preprocess
-            prep = DataPreprocessing(patch_size=self.cfg.patch_size, res_mult=self.cfg.res_mult)
-            subj_viz = prep(subj_viz)
-            self.train_monitor_data = {
-                'id': ref_subj,
-                'mri': subj_viz['mri'][tio.DATA].unsqueeze(0),
-                'ct': subj_viz['ct'][tio.DATA].unsqueeze(0),
-                'orig_shape': subj_viz['original_shape'].tolist(),
-                'pad_offset': subj_viz['pad_offset']
-            }
-
         # 5. Train Loader (Queue)
         train_objs = _make_subj_list(self.train_subjects)
         use_safety = (self.cfg.model_type.lower() == "cnn" and self.cfg.enable_safety_padding)
@@ -462,13 +464,15 @@ class Trainer:
         if self.cfg.anatomix_weights == "v1":
             self.cfg.res_mult = 16 
             self.feat_extractor = Unet(3, 1, 16, 4, 16).to(self.device)
-            ckpt = os.path.join(self.cfg.root_dir, "anatomix", "model-weights", "anatomix.pth")
+            # ckpt = os.path.join(self.cfg.root_dir, "anatomix", "model-weights", "anatomix.pth")
+            ckpt = "/home/minsukc/MRI2CT/anatomix/model-weights/anatomix.pth"
         elif self.cfg.anatomix_weights == "v2":
             self.cfg.res_mult = 32
             self.feat_extractor = Unet(3, 1, 16, 5, 20, norm="instance", interp="trilinear", pooling="Avg").to(self.device)
             # Optimize inference speed
             self.feat_extractor = torch.compile(self.feat_extractor, mode="default")
-            ckpt = os.path.join(self.cfg.root_dir, "anatomix", "model-weights", "best_val_net_G.pth")
+            # ckpt = os.path.join(self.cfg.root_dir, "anatomix", "model-weights", "best_val_net_G.pth")
+            ckpt = "/home/minsukc/MRI2CT/anatomix/model-weights/best_val_net_G.pth"
         else:
             raise ValueError("Invalid anatomix_weights")
             
@@ -506,12 +510,24 @@ class Trainer:
             params.append({'params': self.feat_extractor.parameters(), 'lr': self.cfg.lr_feat_extractor})
             
         self.optimizer = torch.optim.Adam(params)
+        
+        # Auto-Pilot Scheduler
+        # patience=10: Waits 10 validation checks (10 * 5 = 50 epochs) before dropping.
+        # factor=0.5: When stuck, cuts LR in half.
+        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
+            self.optimizer, 
+            mode='min', 
+            factor=0.5, 
+            patience=5, 
+            min_lr=1e-6,
+        )
+
         self.loss_fn = CompositeLoss(weights={
             "l1": self.cfg.l1_w, "l2": self.cfg.l2_w, 
             "ssim": self.cfg.ssim_w, "perceptual": self.cfg.perceptual_w
         }).to(self.device)
         self.scaler = torch.cuda.amp.GradScaler()
-
+        
     def _load_resume(self):
         if not self.cfg.resume_wandb_id: return
         
@@ -565,31 +581,31 @@ class Trainer:
     # VISUALIZATION
     # ==========================================
     def _log_aug_viz(self, epoch):
-        """
-        Simulates augmentation on the monitor subject and logs it.
-        """
-        if not self.train_monitor_data: return
         try:
-            # Reconstruct Subject from Tensor
-            tm = self.train_monitor_data
-            subj = tio.Subject(
-                mri=tio.ScalarImage(tensor=tm['mri'][0].cpu()),
-                ct=tio.ScalarImage(tensor=tm['ct'][0].cpu())
-            )
+            # 1. Get Data
+            subj_id = self.val_subjects[0] 
+            paths = get_subject_paths(self.cfg.root_dir, subj_id)
             
-            # Apply Augmentation
-            aug_transform = get_augmentations()
-            aug = aug_transform(subj)
+            subj = tio.Subject(mri=tio.ScalarImage(paths['mri']), ct=tio.ScalarImage(paths['ct']))
+            prep = DataPreprocessing(patch_size=self.cfg.patch_size, res_mult=self.cfg.res_mult)
+            subj = prep(subj)
+
+            # 2. Augment
+            aug = get_augmentations()(subj)
             hist_str = " | ".join([t.name for t in aug.history])
 
-            # Viz Middle Slice
+            # 3. Slice & Plot
             z = subj['mri'].shape[-1] // 2
+            
+            # NOTE: If aug changes shape, this line will crash.
             orig_sl = subj['mri'].data[0, ..., z].numpy()
             aug_sl = aug['mri'].data[0, ..., z].numpy()
 
             fig, ax = plt.subplots(1, 3, figsize=(10, 4))
-            ax[0].imshow(np.rot90(orig_sl), cmap='gray', vmin=0, vmax=1); ax[0].set_title("Original")
+            ax[0].imshow(np.rot90(orig_sl), cmap='gray', vmin=0, vmax=1); ax[0].set_title(f"Original")
             ax[1].imshow(np.rot90(aug_sl), cmap='gray', vmin=0, vmax=1); ax[1].set_title(f"Augmented\n{hist_str}")
+            
+            # The simple Diff you wanted
             ax[2].imshow(np.rot90(aug_sl - orig_sl), cmap='seismic', vmin=-0.5, vmax=0.5); ax[2].set_title("Diff")
             
             wandb.log({"val/aug_viz": wandb.Image(fig)}, step=epoch)
@@ -597,6 +613,7 @@ class Trainer:
         except Exception as e:
             print(f"[WARNING] Aug Viz failed: {e}")
 
+    
     @torch.no_grad()
     def _visualize_full(self, pred, ct, mri, feats_mri, subj_id, shape, epoch, idx, offset=0):
         """
@@ -699,6 +716,54 @@ class Trainer:
             wandb.log({f"viz/{'train' if idx==-1 else ('val_'+ str(idx))}": wandb.Image(fig)}, step=epoch)
         plt.close(fig)
 
+    @torch.no_grad()
+    def _visualize_lite(self, pred, ct, mri, subj_id, shape, epoch, idx, offset=0):
+        """
+        Lightweight visualization: MRI, GT, Pred, Residual. 
+        """
+        # 1. Unpad Volumes
+        w, h, d = shape
+        gt_ct = unpad(ct, shape, offset).cpu().numpy().squeeze()
+        gt_mri = unpad(mri, shape, offset).cpu().numpy().squeeze()
+        pred_ct = unpad(pred, shape, offset).cpu().numpy().squeeze()
+        
+        # 2. Define Items (Standard 4-column view)
+        items = [
+            (gt_mri, "GT MRI", "gray", (0,1)),
+            (gt_ct, "GT CT", "gray", (0,1)),
+            (pred_ct, "Pred CT", "gray", (0,1)),
+            (pred_ct - gt_ct, "Residual", "seismic", (-0.5, 0.5)),
+        ]
+
+        # 3. Plotting
+        D_dim = gt_ct.shape[-1]
+        num_cols = len(items)
+        slice_indices = np.linspace(0.1 * D_dim, 0.9 * D_dim, 5, dtype=int)
+        
+        fig, axes = plt.subplots(len(slice_indices), num_cols, figsize=(3 * num_cols, 3.5 * len(slice_indices)))
+        plt.subplots_adjust(wspace=0.05, hspace=0.15)
+        
+        if len(slice_indices) == 1: axes = axes.reshape(1, -1)
+
+        for i, z_slice in enumerate(slice_indices):
+            for j, (data, title, cmap, clim) in enumerate(items):
+                ax = axes[i, j]
+                im = ax.imshow(data[:, :, z_slice], cmap=cmap, vmin=clim[0], vmax=clim[1])
+                
+                if title == "Residual": res_im = im
+                if i == 0: ax.set_title(title)
+                ax.axis("off")
+
+        if 'res_im' in locals():
+            cbar = fig.colorbar(res_im, ax=axes[:, 3], fraction=0.04, pad=0.01)
+            cbar.set_label("Residual Error")
+        
+        fig.suptitle(f"Viz LITE â€” {subj_id} (Ep {epoch})", fontsize=16, y=0.99)
+        
+        if self.cfg.wandb:
+            wandb.log({f"viz/{('val_'+ str(idx))}": wandb.Image(fig)}, step=epoch)
+        plt.close(fig)
+        
     # Added method to visualize training patches
     @torch.no_grad()
     def _log_training_patch(self, mri, ct, pred, epoch, step):
@@ -826,8 +891,24 @@ class Trainer:
             subj_id = batch['subj_id'][0]
             pad_offset = batch['pad_offset'][0].item() if 'pad_offset' in batch else 0
             
-            feats = self.feat_extractor(mri)
-            pred = self.model(feats)
+            # Sliding Window (Lite) vs Full Volume (Standard)
+            feats = None
+            if self.cfg.val_sliding_window:
+                def combined_forward(x):
+                    return self.model(self.feat_extractor(x))
+
+                pred = sliding_window_inference(
+                    inputs=mri, 
+                    roi_size=(self.cfg.patch_size, self.cfg.patch_size, self.cfg.patch_size), 
+                    sw_batch_size=self.cfg.val_sw_batch_size, 
+                    predictor=combined_forward,
+                    overlap=self.cfg.val_sw_overlap,
+                    mode="gaussian",
+                    device=self.device
+                )
+            else:
+                feats = self.feat_extractor(mri)
+                pred = self.model(feats)
             
             # Metrics
             pred_unpad = unpad(pred, orig_shape, pad_offset)
@@ -835,32 +916,22 @@ class Trainer:
             met = compute_metrics(pred_unpad, ct_unpad)
             
             # Loss (Composite)
-            l_val, _ = self.loss_fn(pred, ct, feat_extractor=self.feat_extractor)
+            l_val, _ = self.loss_fn(pred, ct, feat_extractor=self.feat_extractor, use_sliding_window = self.cfg.val_sliding_window)
             met['loss'] = l_val.item()
             
             for k, v in met.items():
                 val_metrics[k].append(v)
             
-            # Full Viz
+            # Viz
             if self.cfg.wandb and i < self.cfg.viz_limit:
-                # tqdm.write(f"   ðŸ”Ž Val Viz [{i+1}/{self.cfg.viz_limit}] ({subj_id})")
-                self._visualize_full(pred, ct, mri, feats, subj_id, orig_shape, epoch, idx=i, offset = pad_offset)
-
-        # 2. Train Monitor
-        if self.cfg.wandb and self.train_monitor_data:
-            tm = self.train_monitor_data
-            tm_mri, tm_ct = tm['mri'].to(self.device), tm['ct'].to(self.device)
-            tm_feats = self.feat_extractor(tm_mri)
-            tm_pred = self.model(tm_feats)
-
-            tm_offset = tm['pad_offset'] if 'pad_offset' in tm else 0
-            
-            # Full Viz for Train Sample
-            self._visualize_full(tm_pred, tm_ct, tm_mri, tm_feats, f"{tm['id']}_TRAIN", tm['orig_shape'], epoch, idx=-1, offset = tm_offset)
-            
-            # Aug Viz
-            if self.cfg.augment:
-                self._log_aug_viz(epoch)
+                if self.cfg.val_sliding_window:
+                    self._visualize_lite(pred, ct, mri, subj_id, orig_shape, epoch, idx=i, offset=pad_offset)
+                else:
+                    self._visualize_full(pred, ct, mri, feats, subj_id, orig_shape, epoch, idx=i, offset=pad_offset)
+        
+        # 2. Augmentation Viz
+        if self.cfg.wandb and self.cfg.augment:
+             self._log_aug_viz(epoch)
 
         # 3. Log
         avg_met = {k: np.mean(v) for k, v in val_metrics.items()}
@@ -895,26 +966,32 @@ class Trainer:
             
             loss, comps, gn = self.train_epoch(epoch)
             
+            if epoch % self.cfg.val_interval == 0 or (epoch+1) == self.cfg.total_epochs:
+                avg_met = self.validate(epoch)
+                val_loss = avg_met.get('loss', 0)
+
+                self.scheduler.step(val_loss)
+                
+                tqdm.write(
+                    f"Ep {epoch} | Train: {loss:.4f} | Val: {val_loss:.4f} | "
+                    f"SSIM: {avg_met.get('ssim',0):.4f} | PSNR: {avg_met.get('psnr',0):.2f} | "
+                    f"Bone: {avg_met.get('bone_dice',0):.4f}"
+                )
+
             ep_duration = time.time() - ep_start
             cumulative_time = time.time() - self.global_start_time
-            
+
             if self.cfg.wandb:
+                current_lr = self.optimizer.param_groups[0]['lr']
                 log = {
                     "train_loss/total": loss, 
                     "info/grad_norm": gn, 
                     "info/epoch_duration": ep_duration,
-                    "info/cumulative_time": cumulative_time
+                    "info/cumulative_time": cumulative_time,
+                    "info/lr": current_lr 
                 }
                 for k, v in comps.items(): log[k.replace("loss_", "train_loss/")] = v
                 wandb.log(log, step=epoch)
-            
-            if epoch % self.cfg.val_interval == 0 or (epoch+1) == self.cfg.total_epochs:
-                avg_met = self.validate(epoch)
-                
-                tqdm.write(
-                    f"Ep {epoch} | Train: {loss:.4f} | Val: {avg_met.get('loss',0):.4f} | "
-                    f"SSIM: {avg_met.get('ssim',0):.4f} | PSNR: {avg_met.get('psnr',0):.2f}"
-                )
                 
             if epoch % self.cfg.model_save_interval == 0:
                 self.save_checkpoint(epoch)
@@ -928,7 +1005,8 @@ class Trainer:
 # ==========================================
 DEFAULT_CONFIG = {
     # System
-    "root_dir": "/home/minsukc/MRI2CT",
+    # "root_dir": "/home/minsukc/MRI2CT",
+    "root_dir": "/gpfs/accounts/jjparkcv_root/jjparkcv98/minsukc/MRI2CT/SynthRAD_combined/3.0x3.0x3.0mm", 
     "log_dir": "/gpfs/accounts/jjparkcv_root/jjparkcv98/minsukc/MRI2CT/wandb_logs",
     "seed": 42,
     "device": "cuda" if torch.cuda.is_available() else "cpu",
@@ -941,9 +1019,9 @@ DEFAULT_CONFIG = {
     "val_split": 0.1,
     "augment": True,
     "patch_size": 96,
-    "patches_per_volume": 50,
-    "data_queue_max_length": 200,
-    "data_queue_num_workers": 2,
+    "patches_per_volume": 40,
+    "data_queue_max_length": 400,
+    "data_queue_num_workers": 4,
     "anatomix_weights": "v2", # "v1", "v2"
     "res_mult": 32 ,
     "analyze_shapes": True, 
@@ -951,141 +1029,62 @@ DEFAULT_CONFIG = {
     # Training
     "lr": 3e-4,
     "val_interval": 1,
-    "sanity_check": False,
-    "accum_steps": 32,
+    "sanity_check": True,
+    "accum_steps": 2,
     "model_save_interval": 50,
-    "viz_limit": 2,
+    "viz_limit": 6,
     "viz_pca": False,
-    "steps_per_epoch": 1000,
+    "steps_per_epoch": 25,
     "finetune_feat_extractor": False,
     "lr_feat_extractor": 1e-5,
     
     # Model Choice
     "model_type": "cnn",
-    "model_compile_mode": "default", # "default", "reduce-overhead", None
-    "total_epochs": 500,
+    "model_compile_mode": None, # "default", "reduce-overhead", None
+    "total_epochs": 5001,
     "dropout": 0,
     
     # CNN Specifics
-    "batch_size": 1,
+    "batch_size": 4,
     "cnn_depth": 9,
     "cnn_hidden": 128,
     "final_activation": "sigmoid",
     "enable_safety_padding": True,
+
+    # Sliding Window & Viz Options
+    "val_sliding_window": True, 
+    "val_sw_batch_size": 4, 
+    "val_sw_overlap": 0.25, 
     
     # Loss Weights
     "l1_w": 1.0,
     "l2_w": 0.0,
     "ssim_w": 1.0,
-    "perceptual_w": 0.0, 
+    "perceptual_w": 0.0,
 
     "wandb_note": "test_run",
     "resume_wandb_id": None, 
 }
 
-# EXPERIMENT_CONFIG = [
-#     {
-#         "total_epochs": 5000,
-#         "accum_steps": 2,
-#         "batch_size": 4,
-#         "steps_per_epoch": 25,
-#         # "anatomix_weights": "v1",
-#         # "val_interval": 10,
-#         "val_interval": 5,
-#         "sanity_check": False,
+EXPERIMENT_CONFIG = [
+    {
+        "total_epochs": 5000,
+        "sanity_check": False,
         
-#         "patches_per_volume": 30,
-#         "data_queue_max_length": 500,
-#         "data_queue_num_workers": 4,
-
-#         "wandb_note": "test_run_long (new augmentation + val sliding window inference)",
-#         # "augment": False,
+        "accum_steps": 4,
+        "batch_size": 4,
+        "steps_per_epoch": 100,
+        "val_interval": 1,
+        "viz_limit": 10,
         
-#         # "resume_wandb_id": "l2rpr7g5", 
-#     },
-# ]
-
-EXPERIMENT_CONFIG = [
-#     # ==========================================================
-#     # GROUP 1: THE ANCHOR (BASELINE)
-#     # ==========================================================
-#     {
-#         "wandb_note": "01_Ref_BS1_Acc4_W4 | GOAL: Establish baseline samples/sec and VRAM floor. HYPOTHESIS: Safest but slowest due to lack of parallelism.",
-#         "total_epochs": 10, "steps_per_epoch": 50,
-#         "batch_size": 1, "accum_steps": 4, 
-#         "data_queue_num_workers": 4, "patches_per_volume": 50,
-#         "model_compile_mode": "default", "data_queue_max_length": 200,
-#     },
-
-#     # ==========================================================
-#     # GROUP 2: GPU PARALLELISM (VRAM TRADE-OFF)
-#     # ==========================================================
-#     {
-#         "wandb_note": "02_Speed_BS4_Acc1 | GOAL: Measure raw throughput gain from 4x parallelism. HYPOTHESIS: Significant speedup, but risk of CUDA OOM.",
-#         "total_epochs": 10, "steps_per_epoch": 50,
-#         "batch_size": 4, "accum_steps": 1, 
-#         "data_queue_num_workers": 4, "patches_per_volume": 50,
-#         "model_compile_mode": "default", "data_queue_max_length": 200,
-#     },
-#     {
-#         "wandb_note": "03_Hybrid_BS2_Acc2 | GOAL: Find 'Goldilocks' zone for VRAM efficiency. HYPOTHESIS: Balanced speed increase with lower VRAM risk than BS4.",
-#         "total_epochs": 10, "steps_per_epoch": 50,
-#         "batch_size": 2, "accum_steps": 2, 
-#         "data_queue_num_workers": 4, "patches_per_volume": 50,
-#         "model_compile_mode": "default", "data_queue_max_length": 200,
-#     },
-
-#     # ==========================================================
-#     # GROUP 3: COMPILATION (CPU VS GPU BOTTLENECK)
-#     # ==========================================================
-#     {
-#         "wandb_note": "04_Compile_None | GOAL: Measure cost of JIT overhead. HYPOTHESIS: If speed is same as Ref, 'default' compilation is useless for this small CNN.",
-#         "total_epochs": 10, "steps_per_epoch": 50,
-#         "batch_size": 1, "accum_steps": 4,
-#         "model_compile_mode": None, 
-#         "data_queue_num_workers": 4, "patches_per_volume": 50,
-#         "data_queue_max_length": 200,
-#     },
-#     {
-#         "wandb_note": "05_Compile_ReduceOverhead | GOAL: Test CUDA Graphs for launch-lag. HYPOTHESIS: Fastest iteration time, but creates the highest VRAM spikes.",
-#         "total_epochs": 10, "steps_per_epoch": 50,
-#         "batch_size": 1, "accum_steps": 4,
-#         "model_compile_mode": "reduce-overhead", 
-#         "data_queue_num_workers": 4, "patches_per_volume": 50,
-#         "data_queue_max_length": 200,
-#     },
-
-#     # ==========================================================
-#     # GROUP 4: CPU WORKER SCALING
-#     # ==========================================================
-    # {
-    #     "wandb_note": "06_Workers_0 | GOAL: Identify absolute IO/CPU floor. HYPOTHESIS: GPU will be mostly idle (0-10% util), proving dataloader necessity.",
-    #     "total_epochs": 10, "steps_per_epoch": 50,
-    #     "batch_size": 2, "accum_steps": 2,
-    #     "data_queue_num_workers": 0, 
-    #     "patches_per_volume": 50, "model_compile_mode": "default",
-    #     "data_queue_max_length": 200,
-    # },
-    
-#     {
-#         "wandb_note": "07_Workers_8 | GOAL: Test CPU process overhead. HYPOTHESIS: If GPU util doesn't increase over Ref, W8 is a waste of system RAM.",
-#         "total_epochs": 10, "steps_per_epoch": 50,
-#         "batch_size": 1, "accum_steps": 4,
-#         "data_queue_num_workers": 8, 
-#         "patches_per_volume": 50, "model_compile_mode": "default",
-#         "data_queue_max_length": 200,
-#     },
     
-#     # ==========================================================
-#     # GROUP 5: DISK IO EFFICIENCY
-#     # ==========================================================
-#     {
-#         "wandb_note": "08_Patches100 | GOAL: Measure NIfTI read/augment overhead. HYPOTHESIS: Faster samples/sec by dividing disk load time across 100 patches.",
-#         "total_epochs": 10, "steps_per_epoch": 50,
-#         "batch_size": 1, "accum_steps": 4,
-#         "data_queue_num_workers": 4, "patches_per_volume": 100, 
-#         "data_queue_max_length": 500, "model_compile_mode": "default",
-#     },
+        # "wandb_note": "long_run_anatomix_v2",
+        
+        "anatomix_weights": "v1",
+        "wandb_note": "long_run_anatomix_v1",
+        
+        # "resume_wandb_id": "l2rpr7g5", 
+    },
 ]
 
 if __name__ == "__main__":
